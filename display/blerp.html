<HTML>
   <head>
      <link rel="stylesheet" type="text/css" href="style.css" />
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
   </head>
   <body>
      <section class="section">
         <div class="container">
            <div class="box">
               <h1 class="title">
                  DIFF_IDENTIFIER
               </h1>
               <p class="subtitle">
                  <a href='ICLR_LINK'>Link to ICLR forum</a>
            </div>
            <br />
            <div class="card">
               <footer class="card-footer">
                  <p class="card-footer-item">
                     <span>
                     <a href="PREV_INDEX.html">
                     Previous diff
                     </a>
                     </span>
                  </p>
                  <p class="card-footer-item">
                     <span>
                     <a href="NEXT_INDEX.html">
                     Next diff
                     </a>
                     </span>
                  </p>
               </footer>
		<div class="card-content">
                  <p class="title">
                     ENHANCING VISUAL REPRESENTATIONS FOR EFFI CIENT OBJECT RECOGNITION DURING ONLINE DISTIL LATION Anonymous authors Paper under doubleblind review A BSTRACT We propose ENVISE , an online distillation framework that ENhances VISual representations for Efficient object recognition . We are motivated by the observation that in many realworld scenarios , the probability of occurrence of all classes is not the same and only a subset of classes occur frequently . Exploiting this fact , we aim to reduce the computations of our framework by employing a binary student network ( BSN ) to learn the frequently occurring classes using the pseudolabels generated by the teacher network ( TN ) on an unlabeled image stream . To maintain overall accuracy , the BSN must also accurately determine when a rare ( or unknown ) class is present in the image stream so that the TN can be used in such cases . To achieve this , we propose an attention triplet loss which ensures that the BSN emphasizes the same semantically meaningful regions of the image as the TN . When the prior class probabilities in the image stream vary , we demonstrate that the BSN adapts to the TN faster than the realvalued student network . We also introduce Gain in Efficiency ( GiE ) , a new metric which estimates the relative reduction in FLOPS based on the number of times the BSN and TN are used to process the image stream . We benchmark CIFAR - 100 and tiny - imagenet datasets by creating meaningful inlier ( frequent ) and outlier ( rare ) class pairs that mimic realworld scenarios . We show that ENVISE outperforms state - of - the - art ( SOTA ) outlier detection methods in terms of GiE , and also achieves greater separation between inlier and outlier classes in the feature space . 1 I NTRODUCTION Deep CNNs that are widely used for image classification ( Huang et al. ( 2017 ) ) often require large computing resources and process each image with high computational complexity ( Livni et al . ( 2014 ) ) . In real - world scenarios , the prior probability of occurrence of individual classes in an image stream is often unknown and varies with the deployed environment . For example , in a zoo , the image stream input to the deep CNN will mostly consist of animals , while that of vehicles would be rare . Other object classes such as furniture and aircraft would be absent . Therefore , only a subset of the many classes known to a deep CNN may be presented to it for classification during its deployment . To adapt to the varying prior class probability in the deployed scenario with high efficiency , we propose an online distillation framework - ENVISE . Here , we employ a high capacity general purpose image classifier as the teacher network ( TN ) , while the student network ( SN ) is a low capacity network . For greater efficiency and faster convergence , we require the coefficients of the SN to be binary and refer to it as the binary student network ( BSN ) . When the BSN is first deployed , it is trained on the unlabeled image stream using the predicted labels of the TN as pseudolabels . Once the BSN converges to the performance of the TN , it is used as the primary classifier to rapidly classify the frequent classes faster than the TN . However , if a rare class appears ( i.e. a class absent during online training ) in the image stream , the BSN must accurately detect it as a class it has not yet encountered , which is then processed by the TN . Since the BSN is trained only on the frequent classes , we refer to these classes as inlier ( IL ) and the rare classes as outlier ( OL ) . It is important to note , that the OL classes are outliers with respect to the BSN only , but are known to the TN . Detecting extremely rare classes which are unknown to both the BSN and TN ( global unknowns ) is beyond the scope of this paper . Thus , assuming that the TN knows all possible classes from the deployed environment , we aim to increase the overall efficiency of the system ( without sacrificing performance ) by exploiting the higher probability of occurrence of frequent classes in a given scenario . Our approach for detecting OL classes is motivated from the observation by Ren et al . ( 2019 ) , where networks incorrectly learn to emphasize the background rather than the semantically important regions of the image leading to poor understanding of the IL classes . We know that attention maps highlight the regions of the image responsible for the classifier ’s prediction ( Selvaraju et al . ( 2017 ) ) . <diffdel> In conventional knowledge distillation </diffdel> <diffadd> We empirically observe that </diffadd> , the attention map of the BSN <diffdel> can </diffdel> <diffadd> may </diffadd> focus on the background even when the attention map of the TN emphasizes the semantically meaningful regions of the image . In doing so , the BSN memorizes the labels of the TN , making it difficult to differentiate between the representations of IL and OL classes . To mitigate these issues , we propose an attention triplet loss that achieves two key objectives a ) guide the attention map of the correct prediction of the BSN to focus on the semantically meaningful regions , and b ) simultaneously ensure that the attention map from the correct and incorrect predictions of the BSN are dissimilar . We show that by focusing on the semantically relevant regions of the image , the BSN will learn to distinguish between the representations of IL and OL classes , thereby improving its ability to detect OL classes . To assess the overall gain in efficiency of ENVISE , we propose a new evaluation metric - GiE , based on the number of times the BSN and TN are used to process the image stream . Since the deployed scene is comprised mostly of IL classes with few OL classes , we expect the BSN to be employed most of the time for classifying IL classes . The TN is used rarely i.e. only when the BSN detects an OL class . We refer to efficiency as the overall reduction in FLOPs in the online distillation framework to process the varying prior probability of classes in the image stream . This term differs from conventional model compression techniques ( Frankle & Carbin ( 2019 ) ; Chen et al. ( 2020 ) ) which process the image stream comprising of classes with equal probability using a single compressed model . To the best of our knowledge , we are the first to propose supervision on attention maps for OL detection , and a new evaluation metric that measures the gain in computational efficiency of an online distillation framework . A summary of our main contributions is : • Faster convergence of BSN : We theoretically justify and empirically illustrate that the BSN adapts to the performance of the TN faster than the realvalued SN ( RvSN ) . We also demonstrate the faster convergence of the BSN for different BSN architectures over its corresponding RvSN . • Attention triplet loss ( Lat ) , which guides the BSN to focus on the semantically meaningful regions of the image , thereby improving OL detection . • New evaluation metric - GiE to measure the overall gain in computational efficiency of the online distillation framework , and • We benchmark CIFAR - 100 and tiny - imagenet datasets with SOTA OL detection methods by creating meaningful IL and OL class pairs . ENVISE outperforms these baseline methods for OL detection , improves separation of IL and OL classes in the feature space , and yields the highest gain in computational efficiency . 2 R ELATED W ORK Online distillation : Distilling knowledge to train a lowcapacity student network from a highcapacity teacher network has been proposed as part of model compression ( Hinton et al . ( 2015 ) ) . Wang & Yoon ( 2020 ) provide a detailed review of different knowledge distillation methods . Mullapudi et al. ( 2019 ) propose an online distillation framework for semantic segmentation in videos , while Abolghasemi et al . ( 2019 ) use knowledge distillation to augment a visuomotor policy from visual attention . Lin et al. ( 2019 ) propose an ensemble student network that recursively learns from the teacher network in closed loop manner while Kim et al . ( 2019 ) use a feature fusion module to distill knowledge . Gao et al. ( 2019 ) propose online mutual learning and Cioppa et al. ( 2019 ) propose to periodically update weights to train an ensemble of student networks . These ensemble based methods require large computational resources and are expensive to train . However , ENVISE involves training a compact model that mimics the performance of the TN with less computation . Outlier detection : Outlier detection or out - of - distribution detection ( OOD ) refers to detecting a sample from an unknown class ( Hendrycks & Gimpel ( 2016 ) ) . Existing SOTA OOD methods use outlier samples during training or validation . Yu & Aizawa ( 2019 ) increase the distance between IL and OL samples during training while Vyas et al . ( 2018 ) ; Lee et al. ( 2018 ) add perturbations onto test images and train their network to differentiate between IL and OL samples . However in ENVISE , the BSN is adaptively trained only on IL class images without any knowledge of OL class images , which is a more realistic scenario . Without using outlier classes during training , Hendrycks & Gimpel ( 2016 ) ; Papadopoulos et al. ( 2019 ) ; Bendale & Boult ( 2016 ) ; Hendrycks et al. ( 2019 ) employ class probabilities or measure the similarity between IL and OL features . Yoshihashi et al. ( 2019 ) generate discriminative feature space using reconstruction based approaches . However , the likelihood of correct classification by these networks is based on background regions as described in Ren et al . ( 2019 ) . Hence , these methods fail when applied to real - world IL and OL class pairs since the confidence of prediction is based on background information . We overcome this drawback by proposing supervision on attention maps where we guide the BSN to focus on the important semantic regions using the attention map of the teacher network as a reference . This enables ENVISE to classify IL images with high confidence while also improving OL detection . Teacher network ( pretrained over all classes ) 1 attention map & labels from teacher network Use teacher network 2 attention maps & pseudo labels 2 Input Adaption module ( runtime learning ) Loss Binary student network ( in operation ) 𝛼! , # Yes 4 Outlier detector No 3 𝑓 ( 𝑔 ( 𝑤! , # ) ) 𝑤! , # Output during learning Input Image Converged Binary weights Loss Real - valued weights Outlier ? Binary CNN Training ( a ) Inference . 3 Fast decision 𝑏! , # = 𝑠𝑖𝑔𝑛( 𝑤! , # ) ( b ) Figure 1 : ( a ) The ENVISE architecture adaptively trains the BSN from predictions of the TN , ( b ) The process updates real valued weights that minimize the error produced by their binarized version . 3 O NLINE DISTILLATION FOR EFFICIENT OBJECT RECOGNITION The framework of ENVISE is shown in Figure 1 ( a ) . Initially , the TN ( indicated by “ 1 ” in the figure ) classifies the images from the input stream with high accuracy , albeit at a slower rate . The BSN ( red dotted box ) comprises of an adaptation module ( “ 2 ” ) and a binary CNN ( “ 3 ” ) . Given an image stream , the adaptation module learns the optimum binary weights to mimic the performance of the TN . Once its accuracy converges to that of the TN , the adaption of the realvalued weights stops , but the inference with binary weights continues . The OL detector ( “ 4 ” ) uses the softmax output of the BSN as confidence to either generate the final prediction ( if the confidence is high ) , or treats the image as an OL class and redirects it to the TN for classification . Binarization algorithm for student network : During adaptive training , the BSN does not have access to the labeled data and mimics the behavior of the TN on the IL classes as shown in Figure 1 ( b ) . For an input image x , let f ( g( wi , j ) , x ) represent the output of the BSN where wi , j are real - valued weights of ith kernel in the j th layer , and g(. ) is a binarizing function for these weights . During adaptive training , the error between the output of the BSN and TN is minimized by optimizing ∗ wi , j = arg ming ( wi , j ) kLk , where L is the overall loss function used for training the BSN . While the intermediate weight wi , j is computed during adaptive learning , the binary versions of these ∗ weights wi , j = g( wi , j ) are used for fast computation . After convergence , the real valued weights are discarded , and only the binary weights are used . The question becomes what is a good choice for the binarization function g(. ) ? Following Rastegari et al. ( 2016 ) , we find a binary vector bi , j ( whose ∗ elements are either + 1 or − 1 ) , and a non-negative scalar αi , j such that wi , j = αi , j . bi , j is the best 1 approximation of wi , j in a minimum squared error sense , where αi , j = N |wi , j | 1 , bi , j = sign ( wi , j ) . However , unlike Rastegari et al. ( 2016 ) where a standalone network is binarized , our BSN learns the ∗ wi , j during adaptive training in an online distillation framework . In doing so , the BSN compensates for the effects of binarization on the overall loss function . Our motivation for using the BSN is that it classifies the image stream with high accuracy and utilizes fewer FLOPs as compared to the RvSN . This results in an increase in efficiency of the overall framework . Furthermore , the BSN converges to the performance of the TN faster than the RvSN , which we theoretically emphasize in the Lemma below and experimentally validate in Section 4 . <diffdel> The </diffdel> <diffadd> We provide the sketch </diffadd> proof <diffdel> of </diffdel> <diffadd> here and </diffadd> the <diffdel> Lemma is presented </diffdel> <diffadd> complete proof </diffadd> in <diffdel> the </diffdel> Appendix A.1 . Lemma 3 . 1 . Let RvSN and BSN represent the real - valued student network and binary student network respectively with the same network architecture . Let R (. ) denote the rate of convergence in terms of accuracy when the student network is adaptively trained using the pseudolabels from the teacher network . Then , R ( BSN ) > R ( RvSN ) for the same image stream and number of <diffadd> . Proof : We assume our image stream as x ( n ) where n = 1 , 2 , 3 ... N , comprises of N samples from the deployed scenario . Since the weights of the BSN are derived from the RvSN , we prove this lemma first for RvSN and then extend it to the BSN . Let w∗ be the optimal weight value which represents network convergence i.e. misclassification error = 0 . The weight update rule using back propagation is given by : w( n + 1 ) = w ( n ) + ηx ( n ) [ Since η = 1 ] w( n + 1 ) = w ( n ) + x ( n ) ( 1 ) Computing the bounds of the weights of the RvSN , we have n2 α 2 kw ∗ k 2 2 ≤ kw ( n + 1 ) k ≤ nβ ( 2 ) 2 where α = argminx ( n ) ∈C w? x ( n ) and β = argmaxx ( n ) ∈C kx ( k ) k . From eq. 2 , we can say that for this inequality to be satisfied , there exists n?r which denotes the optimal number of samples for which the RvSN converges i.e. we obtain w ? at n = n?r . This is given as follows : 2 n?r α 2 2 kw ? k = n ?r β 2 n?r ( 3 ) β kw ? k = α 2 ∗ 2 k Thus from eq. 3 , we can say that the RvSN achieves convergence after being trained on βkw α 2 number of samples . Since , the weights of the BSN are derived from the RvSN , we substitute the value of binary weight ŵ∗ from Rastegari et al . ( 2016 ) to obtain n? b = n ?r N ( 4 ) Here , n? b is the optimal number of samples for which the BSN converges . Comparing eq. 3 and eq. 4 , we observe that number of n∗b < n?r i.e. the BSN takes fewer samples to converge to the performance of the TN than RvSN given the same network architecture </diffadd> iterations . Knowledge transfer from teacher to student network : Given an unlabeled image stream , the BSN is trained P from the predictions of the TN as hard pseudolabels using cross-entropy loss , as Ld = − N1 i yi log ( si ) . Here , y and s are the pseudo - labels generated by the TN and the softmax output of the BSN respectively , N is the total images in the image stream . Once the BSN converges to the performance of the TN , we employ the BSN as the primary classifier during inference . When the deployed scenario does not change for a long duration , the BSN classifies the IL classes faster than the TN without relying on the latter . This improves the overall efficiency of the framework as the TN is now used to classify only the OL classes ( number of OL images << number of IL images ) . However , to maintain overall accuracy , the BSN must also accurately determine when the image from the input stream belongs to an OL class so that the TN can be used in such cases <diffadd> We know that attention maps highlight the regions of the image responsible for the classifier ’s prediction ( Li et al . ( 2018 ) ) . Some examples of the attention maps from the correct predictions of the BSN and the TN are shown in Figure 2 ( a ) . The first three columns in Figure 2 ( a ) show that although the BSN ( adaptively trained using Ld ) and the TN both correctly classify the images , their attention maps are significantly different . The attention map from the correct prediction of the BSN </diffadd> . Input Image Attention map TN Attention map BSN from highest prediction Attention map BSN from second highest prediction Attention map BSN after using 𝐿! " 𝐿! + c = 0.93 c = 0.68 c = 0.21 c = 0.96 c = 0.77 c = 0.42 c = 0.33 c = 0.92 𝐿 " # ( a ) ( b ) Figure 2 : ( a ) The attention map visualization illustrating that after the BSN is trained using the proposed attention triplet loss , it learns to focus on the semantically meaningful regions in the image . The confidence of prediction ( c ) also increases . ( b ) Separation between IL and OL classes in feature space improves with the proposed attention triplet loss . More visualizations in Appendix A.4 <diffdel> We know that attention maps highlight the regions of the image responsible for the classifier ’s prediction ( Li et al . ( 2018 ) ) . Some examples of the attention maps from the correct predictions of the BSN and the TN are shown in Figure 2 ( a ) . The first three columns in Figure 2 ( a ) show that although the BSN ( adaptively trained using Ld ) and the TN both correctly classify the images , their attention maps are significantly different . The attention map from the correct prediction of the BSN </diffdel> focuses on the background while that of the TN lies on the semantically meaningful regions of the image . Furthermore , the attention maps of the BSN ’s correct and incorrect predictions are visually similar which causes the BSN to learn incorrect representations of the IL images while correctly classifying them . To better differentiate between IL and OL classes , the BSN should not memorize the predictions of the TN , but learn proper representations of the IL images . To achieve this , we propose the attention triplet loss given in Equation <diffdel> 1 </diffdel> <diffadd> 5 </diffadd> that causes the BSN ’s attention map to be similar to that of the TN while forcing the attention map from the BSN ’s correct and incorrect prediction to be different . We use Grad - CAM ( Selvaraju et <diffdel> al. </diffdel> <diffadd> al . </diffadd> ( 2017 ) ) to compute the attention map A from the onehot vector of the predicted class . To generate the attention map triplets , we use the attention map from the TN ’s prediction At as the anchor . The hard positive attention map Asp is obtained from correct prediction of the BSN . When the BSN misclassifies an image ( prediction of BSN and TN are different ) , we use the label from the TN to compute Asp . Motivated by the findings in Wang et al. ( 2019 ) , we observe that the attention maps generated from the correct and incorrect predictions of the BSN are visually similar . Hence , we use the attention map from an incorrect prediction as the hard negative attention map Asn to enforce its separability from Asp . To avoid bad local minima and a collapsed model early on during training , mining suitable hard negatives are important ( Schroff et al . ( 2015 ) ) . Hence , we use the attention map from the second most probable class ( incorrect prediction ) as our hard negative due to its similarity with the hard positive ( third and fourth column in Figure 2 ( a ) ) . Thus , we formulate the attention triplet loss Lat as : Lat = <diffdel> N </diffdel> <diffadd> N </diffadd> K 1 1 XX ( Atk − Aspk NK n k − 2 K <diffadd> k </diffadd> X Atk − Asnk <diffdel> k </diffdel> + δ <diffadd> 2 </diffadd> ) ( <diffdel> 1 </diffdel> <diffadd> 5 </diffadd> ) <diffdel> 2 </diffdel> Initially , the hard negative lies within the margin since its squared distance with the hard positive is small . Hence , Lat enforces separation between hard positive and hard negative by a distance greater than the value of margin δ , which we empirically set as 1 . 5 . N is the total number of samples and K is the total number of pixels in A. Using Lat and Ld , we formulate our final objective function as : Lf = λ 1 Ld + λ 2 Lat ( <diffdel> 2 </diffdel> <diffadd> 6 </diffadd> ) where λ 1 and λ 2 are the weights of Ld and Lat which we empirically set as 1 and 0 . 2 respectively . The effect of Lat is shown in the last column of Figure 2 ( a ) where the BSN learns proper representations of the IL image due to its attention map being very similar to that of the TN ( second column ) . Furthermore , the improvement in feature space separation between the IL and OL classes in Figure 2 ( b ) shows that the BSN not only forms clusters of the different IL classes ( in red ) , but is also well separated from the OL classes ( in purple ) . 4 E XPERIMENTAL D ETAILS Datasets and implementation : We evaluate ENVISE on CIFAR100 ( Krizhevsky & Hinton ( 2009 ) ) and tiny - imagenet ( TI ) ( Yao & Miller ( 2015 ) ) datasets by creating meaningful IL and OL superclass pairs that mimic real - world scenarios . On the test set of CIFAR - 100 and validation set of TI dataset , we create 12 and 10 super - classes respectively . We summarize our experimental settings in Table 1 . We use DenseNet - 201 ( Huang et al . ( 2017 ) ) as the TN , pre-trained on the training set of all classes of CIFAR100 or the TI dataset individually . The BSN is a VGG16 ( Simonyan & Zisserman ( 2014 ) ) network whose weights , except for the first and last convolution layer , are binarized ( Rastegari et al . ( 2016 ) ) . We also compare this with other choices for the BSN including AlexNet , ResNet - 18 and ResNet - 50 . In the Appendix A.3 , we illustrate that ENVISE is insensitive to the specific TN and BSN architecture used and achieves high performance gains even with different network architectures . Table 1 : Different pairs of IL and OL super - classes on the CIFAR100 and TI datasets . CIFAR - 100 ( Krizhevsky & Hinton ( 2009 ) ) IL \ OL superclasses <diffdel> aquatic animals \ food container ( C1 ) flora \ electrical items ( C2 ) fruits \ furniture ( C3 ) insects \ manmade things ( C4 ) animals \ people ( C5 ) outdoor places \ vehicles ( C6 ) </diffdel> tiny - imagenet ( Yao & Miller ( 2015 ) <diffadd> IL \ outlier superclasses </diffadd> ) # IL # OL # IL # OL # total train train test test <diffadd> aquatic animals \ food container ( C1 ) flora \ electrical items ( C2 ) fruits \ furniture ( C3 ) insects \ manmade things ( C4 ) animals \ people ( C5 ) outdoor places \ vehicles ( C6 ) </diffadd> classes 840 630 350 560 1750 420 0 0 0 0 0 0 1200 900 500 800 2500 600 500 500 500 500 500 1000 17 14 10 13 30 16 <diffdel> IL \ outlier super - classes animals \ garments ( T1 ) reptiles \ edible items ( T2 ) aquatic animals \ birds ( T 3 ) household items \ nature ( T4 ) vehicles \ miscellaneous ( T5 ) </diffdel> # IL # OL # IL # OL # total train train test test <diffadd> animals \ garments ( T1 ) reptiles \ edible items ( T2 ) aquatic animals \ birds ( T 3 ) household items \ nature ( T4 ) vehicles \ miscellaneous ( T5 ) </diffadd> classes 875 630 420 1432 525 0 0 0 0 0 1250 900 600 2050 750 900 800 250 700 1800 43 34 17 55 51 Training and evaluation : Throughout our experiments , we fix the TN and do not train it . We train the BSN using the pseudo - labels and attention map from the TN using the cost function in eq. <diffdel> 2 </diffdel> <diffadd> 6 </diffadd> only on the IL classes of the super - class from Table 1 . This is done using a learning rate of 1e −4 for 10 epochs using the Adam optimizer ( Kingma & Ba ( 2014 ) ) with a batch size of 10 . Once the BSN converges to the performance of the TN , we employ it as our primary classifier during inference . To create a more realistic scenario during inference , we random center crop and randomly rotate the image stream between [ − 15 ◦ , 15 ◦ ] . We assume that the distribution of classes will not change rapidly in the deployed scenario , and that the BSN can be used for inference for long durations ( e.g days , weeks or months ) . Thus , the epochs for online distillation are expected to require a small fraction of that time . For each image from the input stream during inference ( comprising of IL and OL classes ) , following Hendrycks & Gimpel ( 2016 ) , we compute the confidence of prediction from the softmax probability of the predicted class . If the confidence is low , we treat the image as an OL class and transfer it to the TN for classification . <diffdel> Gradually changing the deployed scenario : When the prior probabilities of the classes change in the deployed scenario , the BSN quickly re-trains to learn the new IL classes and regains efficiency on the new image stream . Figure 3 illustrates the learning behavior of the BSN when the IL and 𝐶 ! 𝐶 " </diffdel> 𝐶 ! 𝐶 " 𝐶# 𝐶 $ 𝐶 % 𝐶& 𝐶 ! 𝐶 % 𝐶& 𝐶 ! ( a ) BSN : Binary <diffadd> 𝐶 ! 𝐶 " </diffadd> Alexnet 𝐶# 𝐶 $ 𝐶 " 𝐶# 𝐶 $ 𝐶 % 𝐶& 𝐶 % 𝐶& ( b ) BSN : Binary VGG - 16 ( c ) BSN : Binary ResNet - 18 𝐶 " 𝐶# 𝐶 $ ( d ) BSN : Binary ResNet - 50 Figure 3 : ( a ) : ( d ) The BSN converges to the performance of the TN faster than the RvSN and the variants of BSN <diffdel> ) </diffdel> . The red period illustrates adaptive training and white period illustrates inference . We observe a similar convergence pattern with different binary network architectures <diffadd> Gradually changing the deployed scenario : When the prior probabilities of the classes change in the deployed scenario , the BSN quickly re-trains to learn the new IL classes and regains efficiency on the new image stream . Figure 3 illustrates the learning behavior of the BSN when the IL and </diffadd> . OL classes are changed . Initially , we assume that the image stream is comprised of only IL classes from C1 . Once the BSN ( in green ) converges to the performance of the TN ( in red ) , we stop training the BSN and observe that it achieves an accuracy comparable to the TN ’s accuracy . The learning behaviour of the RvSN ( in blue ) is slower than the BSN and has poorer accuracy after 10 epochs . Directly binarizing the RvSN ( purple line ) is also worse than the BSN , and substantially differs from the binarization algorithm in Section 3 which compensates for the quantization effect . The yellow line shows the performance of the standalone BSN which is not trained during the adaptive training . In Figure 3 we keep the IL / OL superclasses the same for 40 epochs , and then switch the input stream to a different scenario . The 10 epochs shaded in red indicate the adaptive training phase , while the unshaded ( white ) intervals indicate the 30 epochs for inference . We randomly choose our inference phase as 30 epochs . We show in Appendix A.3 that during inference , the combined accuracy of ENVISE is identical to that of the standalone TN , which illustrates that ENVISE maintains the overall accuracy of the system . When the scenario changes after 40 epochs ( e.g from C1 to C2 ) , we observe similar and consistent learning behaviour , and the BSN retrains quickly from the TN to regain efficiency . We also observe a similar convergence pattern for different BSN architectures when adaptively trained from the same TN in Figure 3 . Here , we observe that the rate of convergence for Binary AlexNet is the fastest and that of Binary Resnet50 is the slowest . This illustrates that a smaller binary network would make an ideal BSN in realworld scenarios , since it would utilize fewer epochs during adaptive training . ED CE CS ODIN OECC MCD CAL ENVISE p = 0 . 68 p = 0.55 p = 0.42 p = 0.59 p = 0.064 p = 0.077 p = 0.39 p = 0.041 p = 0.51 p = 0.46 p = 0.40 p = 0.28 p = 0.21 p = 0.37 p = 0.58 p = 0.016 Figure 4 : Feature space separation of the IL and OL classes for different baseline methods . p−value denotes the overlap between the IL and OL features where , lower value indicates better separation . Comparison with SOTA outlier detection methods : The ability to accurately distinguish between IL and OL classes is key for improving the efficiency of ENVISE . We benchmark ENVISE on the CIFAR - 100 and TI datasets with SOTA OL detection methods like error detection ( ED ) ( Hendrycks & Gimpel ( 2016 ) ) , confidence estimation ( CE ) ( DeVries & Taylor ( 2018 ) ) , confidence scaling ( CS ) ( DeVries & Taylor ( 2018 ) ) , ODIN ( Liang et al . ( 2018 ) ) , outlier exposure with confidence control ( OECC ) ( Papadopoulos et al . ( 2019 ) ) , maximum classifier discrepancy ( MCD ) ( Yu & Aizawa ( 2019 ) ) and confidence aware learning ( CAL ) ( Moon et al . ( 2020 ) ) . We use the official code of these methods and adaptively train them using their proposed loss functions on our experimental settings . For fair comparisons in terms of network architecture , we use the TN as DenseNet - 201 and the SN as our binary VGG 16 . Following Hendrycks & Gimpel ( 2016 ) , we use FPR at 95 % True positive rate ( TPR ) , detection error ( DE ) , area under ROC curve ( AuROC ) , and area under precision - recall curve ( AuPR ) as our evaluation metric . From Table 2 , we observe that ENVISE outperforms the best performing baseline method by achieving the lowest FPR and detection error with high AuROC and AuPR. SOTA model compression techniques ( Frankle & Carbin ( 2019 ) ) do not focus on processing images from the input stream with varying prior class probabilities . Hence , direct comparison with these methods is not meaningful since their objectives are different from those of ENVISE . We visualize the separation of IL and OL classes in the feature space using UMAP ( McInnes et al. ( 2018 ) ) across the last fully connected layer of the BSN . Figure 4 shows that ENVISE has the best separation between IL and OL classes which is consistent with the quantitative analysis shown in Table 2 . To quantify the feature separation , we compute the p−value using Wilcoxon ’s rank sum test ( Wilcoxon ( 1992 ) ) for the null hypothesis that the IL and OL feature distribution are the same i.e they overlap . Ideally for high separation , the p-value should be less than 0. 05 ( rejecting the hypothesis with 95 % confidence ) . We observe from Figure 4 that the p-value of ENVISE has the smallest value as compared to the SOTA OL detection methods . This indicates that ENVISE achieves the least overlap between IL and OL classes , thereby outperforming the SOTA OL detection methods in its ability to better detect OL classes . A detailed comparison of ENVISE with each baseline method on different pairs of IL and OL classes and the corresponding feature space visualization is presented in the Appendix A.4 . Furthermore , in Appendix A.2 , we ablate the effect of the proposed Lat , margin ( δ ) with the BSN and also performance of OL detection with RvSN . We present additional discussions in Appendix A.3 . Table 2 : Performance comparison of ENVISE with the best performing baseline method on different IL and OL class pairs on CIFAR100 and TI dataset . ↓ and ↑ indicate smaller and greater value is better respectively . The detailed comparison with each baseline is in the Appendix A.4 Dataset CIFAR - 100 Tiny - Imagenet Group Method \ Metric FPR ( 95 % TPR ) ↓ Detection error ↓ AuROC ↑ AuPR ( IL ) ↑ AuPR ( OL ) ↑ C 1 Best Baseline . ENVISE 0 . 90 0 . 88 0 . 42 0 . 39 0 . 59 0 . 62 0 . 77 0 . 80 0 . 37 0 . 39 C2 Best Baseline ENVISE 0 . 69 0 . 41 0 . 28 0 . 16 0 . 79 0 . 89 0 . 86 0 . 88 0 . 71 0 . 92 C3 Best Baseline ENVISE 0 . 69 0 . 57 0 . 21 0 . 18 0 . 86 0 . 89 0 . 87 0 . 91 0 . 87 0 . 87 C4 Best Baseline ENVISE 0 . 65 0 . 61 0 . 27 0 . 26 0 . 79 0 . 75 0 . 85 0 . 82 0 . 72 0 . 77 C5 Best Baseline ENVISE 0 . 82 0 . 76 0 . 39 0 . 29 0 . 62 0 . 79 0 . 90 0 . 95 0 . 23 0 . 42 C6 Best Baseline ENVISE 0 . 63 0 . 49 0 . 22 0 . 16 0 . 85 0 . 91 0 . 79 0 . 87 0 . 90 0 . 97 T1 Best Baseline . ENVISE 0 . 72 0 . 69 0 . 28 0 . 23 0 . 84 0 . 89 0 . 88 0 . 91 0 . 65 0 . 71 T2 Best Baseline ENVISE 0 . 80 0 . 78 0 . 29 0 . 26 0 . 75 0 . 81 0 . 79 0 . 86 0 . 68 0 . 73 T3 Best Baseline ENVISE 0 . 78 0 . 74 0 . 32 0 . 20 0 . 68 0 . 81 0 . 76 0 . 93 0 . 61 0 . 63 T4 Best Baseline ENVISE 0 . 83 0 . 78 0 . 20 0 . 18 0 . 69 0 . 75 0 . 78 0 . 91 0 . 51 0 . 44 T5 Best Baseline ENVISE 0 . 80 0 . 78 0 . 30 0 . 26 0 . 90 0 . 91 0 . 85 0 . 85. 0 . 79 <diffdel> 0 . </diffdel> <diffadd> 0. </diffadd> <diffadd> ( a ) ( b ) Figure 5 : ( a ) Comparison of ENVISE with SOTA OL detection methods , and ( b ) Comparison of different BSN architectures in terms of GiE on different superclasses of the CIFAR - 100 dataset . </diffadd> 84 Gain in Efficiency ( GiE ) : The main focus of our work is to develop an efficient system to classify the image stream with low computational cost and high accuracy . We propose a new evaluation metric - GiE to measure the overall gain in efficiency from the number of times the BSN and TN are used individually to classify the image stream . During inference , we require the BSN to accurately classify the IL classes such that it does not rely on the TN thereby reducing the overall computation cost . Furthermore , the BSN should detect an image as OL so that the TN can be used in such cases . To process a single image on the CIFAR - 100 dataset , the TN uses 9048 MFLOPs and 28 . 5msec , while <diffdel> ( a ) ( b ) Figure 5 : ( a ) Comparison of ENVISE with SOTA OL detection methods , and ( b ) Comparison of different BSN architectures in terms of GiE on different superclasses of the CIFAR - 100 dataset . </diffdel> the RvSN uses 310 MFLOPs and 3. 04 msec . However , the BSN achieves 6 MFLOPs and 0. 57 msec ; a ∼ 50× reduction in computation and a ∼ 6× speed improvement over the RvSN . Furthermore , the BSN occupies ∼ 30× and ∼ 100× less memory than the RvSN and TN respectively . During adaptive training , since each image is processed by both the BSN and TN , the total FLOPs used is F Ltr = ( X + Y ) , where X and Y are the FLOPs used by the TN and BSN respectively . Once the the BSN converges to the performance of the TN , only the BSN is used to process the input image . Hence , for IL images the total FLOPs is F Lin = Y + ( f p × X ) , where ( f p × X ) indicates the number of times the TN is used when the BSN misclassifies an IL class as an OL . Similarly for OL images , the FLOPs is F Lo = Y + ( od × X ) , where ( od × X ) denotes the OL classes correctly detected by the BSN which is then processed by the TN . Since the adaptive training and inference phase for a given superclass occur for Ntr and Nin epochs respectively , the relative FLOPS of ENVISE compared to the TN is given as   Ntr <diffdel> Nin </diffdel> F <diffadd> Nin </diffadd> Ltr ( p × F Lin + q × F Lo ) RF L = <diffdel> + ( 3 ) </diffdel> <diffadd> + </diffadd> × × NT X NT <diffadd> ( 7 ) </diffadd> X Here , NT = Ntr + Nin . For high computational efficiency , RF L should be small . If the BSN works with perfect accuracy ( i.e. f p = 0 and od = 1 . 0 ) , then the minimum value of RF L during inference is RF Lmin = Y +q×X . We define GiE as RF L normalized with respect to its minimum value , i.e. X <diffdel> Y + q×X </diffdel> <diffadd> GiE = </diffadd> RF Lmin <diffdel> i ( 4 ) </diffdel> =h <diffdel> GiE = </diffdel> Ntr <diffdel> Nin </diffdel> RF <diffadd> NT </diffadd> L × <diffdel> FL </diffdel> <diffadd> F Ltr </diffadd> <diffadd> Y + q×X Nin NT i </diffadd> + × ( p × F <diffdel> L </diffdel> <diffadd> Lin </diffadd> + q × F <diffdel> L </diffdel> <diffadd> Lo </diffadd> ) <diffdel> NT tr NT in o </diffdel> <diffadd> ( 8 ) </diffadd> To numerically compute GiE , we use X = 9048 ( TN ) , Y = 6 ( BSN ) , Ntr = 10 and Nin = 30 , while p and q are calculated from Table 1 . However , as discussed previously , Nin can be a very large value since the inference phase can occur for a very long duration . Figure 5 illustrates that ENVISE outperforms SOTA OL detection methods in terms of GiE for different scenarios on CIFAR100 dataset . Thus , the Lat ensures that the BSN can accurately distinguish between representations of IL and OL classes which enables ENVISE to operate in an efficient manner . 5 C ONCLUSION We propose ENVISE , an online distillation framework which uses a BSN to adaptively learn relevant knowledge from a TN to quickly classify the frequent classes in the deployed scenario . In doing so , it automatically allocates processing resources ( i.e. either the BSN or the TN ) to reduce overall computation . To learn proper representations of the IL classes , we propose an attention triplet loss that enables the BSN to learn the semantically relevant regions of the image that are emphasized by the TN . This enables the BSN to accurately distinguish between representations of IL and OL classes which is key for maintaining overall accuracy . Our experiments show that the BSN i) quickly converges to the performance of the TN , ii ) classifies IL classes more accurately than a RvSN and other variants of the BSN , and iii ) distinguishes between IL and OL classes with lower FPR and detection error than other SOTA OL detection methods on CIFAR100 and tiny - imagenet datasets . We introduce a new metric GiE to assess the overall gain in efficiency , and experimentally show that the attention triplet loss enables ENVISE to achieve higher GiE than SOTA OL detection methods . We show that ENVISE is agnostic to the specific TN and BSN and achieves high gains with different BSN and TN architectures .
                  </p>
               </div>

            </div>
            </p>
         </div>
      </section>
   </body>
</HTML>
