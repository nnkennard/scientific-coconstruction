<HTML>
   <head>
      <link rel="stylesheet" type="text/css" href="style.css" />
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
   </head>
   <body>
      <section class="section">
         <div class="container">
            <div class="box">
               <h1 class="title">
                  DIFF_IDENTIFIER
               </h1>
               <p class="subtitle">
                  <a href='ICLR_LINK'>Link to ICLR forum</a>
            </div>
            <br />
            <div class="card">
               <footer class="card-footer">
                  <p class="card-footer-item">
                     <span>
                     <a href="PREV_INDEX.html">
                     Previous diff
                     </a>
                     </span>
                  </p>
                  <p class="card-footer-item">
                     <span>
                     <a href="NEXT_INDEX.html">
                     Next diff
                     </a>
                     </span>
                  </p>
               </footer>
		<div class="card-content">
                  <p class="title">
                     ENHANCING VISUAL REPRESENTATIONS FOR EFFI CIENT OBJECT RECOGNITION DURING ONLINE DISTIL LATION Anonymous authors Paper under doubleblind review A BSTRACT We propose ENVISE , an online distillation framework that ENhances VISual representations for Efficient object recognition . We are motivated by the observation that in many realworld scenarios , the probability of occurrence of all classes is not the same and only a subset of classes occur frequently . Exploiting this fact , we aim to reduce the computations of our framework by employing a binary student network ( BSN ) to learn the frequently occurring classes using the pseudolabels generated by the teacher network ( TN ) on an unlabeled image stream . To maintain overall accuracy , the BSN must also accurately determine when a rare ( or unknown ) class is present in the image stream so that the TN can be used in such cases . To achieve this , we propose an attention triplet loss which ensures that the BSN emphasizes the same semantically meaningful regions of the image as the TN . When the prior class probabilities in the image stream vary , we demonstrate that the BSN adapts to the TN faster than the realvalued student network . We also introduce Gain in Efficiency ( GiE ) , a new metric which estimates the relative reduction in FLOPS based on the number of times the BSN and TN are used to process the image stream . We benchmark CIFAR - 100 and tiny - imagenet datasets by creating meaningful inlier ( frequent ) and outlier ( rare ) class pairs that mimic realworld scenarios . We show that ENVISE outperforms state - of - the - art ( SOTA ) outlier detection methods in terms of GiE , and also achieves greater separation between inlier and outlier classes in the feature space . 1 I NTRODUCTION Deep CNNs that are widely used for image classification ( Huang et al. ( 2017 ) ) often require large computing resources and process each image with high computational complexity ( Livni et al . ( 2014 ) ) . In real - world scenarios , the prior probability of occurrence of individual classes in an image stream is often unknown and varies with the deployed environment . For example , in a zoo , the image stream input to the deep CNN will mostly consist of animals , while that of vehicles would be rare . Other object classes such as furniture and aircraft would be absent . Therefore , only a subset of the many classes known to a deep CNN may be presented to it for classification during its deployment . To adapt to the varying prior class probability in the deployed scenario with high efficiency , we propose an online distillation framework - ENVISE . Here , we employ a high capacity general purpose image classifier as the teacher network ( TN ) , while the student network ( SN ) is a low capacity network . For greater efficiency and faster convergence , we require the coefficients of the SN to be binary and refer to it as the binary student network ( BSN ) . When the BSN is first deployed , it is trained on the unlabeled image stream using the predicted labels of the TN as pseudolabels . Once the BSN converges to the performance of the TN , it is used as the primary classifier to rapidly classify the frequent classes faster than the TN . However , if a rare class appears ( i.e. a class absent during online training ) in the image stream , the BSN must accurately detect it as a class it has not yet encountered , which is then processed by the TN . Since the BSN is trained only on the frequent classes , we refer to these classes as inlier ( IL ) and the rare classes as outlier ( OL ) . It is important to note , that the OL classes are outliers with respect to the BSN only , but are known to the TN . Detecting extremely rare classes which are unknown to both the BSN and TN ( global unknowns ) is beyond the scope of this paper . Thus , assuming that the TN knows all possible classes from the deployed environment , we aim to increase the overall efficiency of the system ( without sacrificing performance ) by exploiting the higher probability of occurrence of frequent classes in a given scenario . Our approach for detecting OL classes is motivated from the observation by Ren et al . ( 2019 ) , where networks incorrectly learn to emphasize the background rather than the semantically important regions of the image leading to poor understanding of the IL classes . We know that attention maps highlight the regions of the image responsible for the classifier ‚Äôs prediction ( Selvaraju et al . ( 2017 ) ) . <diffdel> In conventional knowledge distillation </diffdel> <diffadd> We empirically observe that </diffadd> <diffdel> can </diffdel> <diffadd> may </diffadd> <diffdel> The </diffdel> <diffadd> We provide the sketch </diffadd> <diffdel> of </diffdel> <diffadd> here and </diffadd> <diffdel> Lemma is presented </diffdel> <diffadd> complete proof </diffadd> <diffdel> the </diffdel> <diffadd> . Proof : We assume our image stream as x ( n ) where n = 1 , 2 , 3 ... N , comprises of N samples from the deployed scenario . Since the weights of the BSN are derived from the RvSN , we prove this lemma first for RvSN and then extend it to the BSN . Let w‚àó be the optimal weight value which represents network convergence i.e. misclassification error = 0 . The weight update rule using back propagation is given by : w( n + 1 ) = w ( n ) + Œ∑x ( n ) [ Since Œ∑ = 1 ] w( n + 1 ) = w ( n ) + x ( n ) ( 1 ) Computing the bounds of the weights of the RvSN , we have n2 Œ± 2 kw ‚àó k 2 2 ‚â§ kw ( n + 1 ) k ‚â§ nŒ≤ ( 2 ) 2 where Œ± = argminx ( n ) ‚ààC w? x ( n ) and Œ≤ = argmaxx ( n ) ‚ààC kx ( k ) k . From eq. 2 , we can say that for this inequality to be satisfied , there exists n?r which denotes the optimal number of samples for which the RvSN converges i.e. we obtain w ? at n = n?r . This is given as follows : 2 n?r Œ± 2 2 kw ? k = n ?r Œ≤ 2 n?r ( 3 ) Œ≤ kw ? k = Œ± 2 ‚àó 2 k Thus from eq. 3 , we can say that the RvSN achieves convergence after being trained on Œ≤kw Œ± 2 number of samples . Since , the weights of the BSN are derived from the RvSN , we substitute the value of binary weight wÃÇ‚àó from Rastegari et al . ( 2016 ) to obtain n? b = n ?r N ( 4 ) Here , n? b is the optimal number of samples for which the BSN converges . Comparing eq. 3 and eq. 4 , we observe that number of n‚àób < n?r i.e. the BSN takes fewer samples to converge to the performance of the TN than RvSN given the same network architecture </diffadd> <diffadd> We know that attention maps highlight the regions of the image responsible for the classifier ‚Äôs prediction ( Li et al . ( 2018 ) ) . Some examples of the attention maps from the correct predictions of the BSN and the TN are shown in Figure 2 ( a ) . The first three columns in Figure 2 ( a ) show that although the BSN ( adaptively trained using Ld ) and the TN both correctly classify the images , their attention maps are significantly different . The attention map from the correct prediction of the BSN </diffadd> <diffdel> We know that attention maps highlight the regions of the image responsible for the classifier ‚Äôs prediction ( Li et al . ( 2018 ) ) . Some examples of the attention maps from the correct predictions of the BSN and the TN are shown in Figure 2 ( a ) . The first three columns in Figure 2 ( a ) show that although the BSN ( adaptively trained using Ld ) and the TN both correctly classify the images , their attention maps are significantly different . The attention map from the correct prediction of the BSN </diffdel> <diffdel> 1 </diffdel> <diffadd> 5 </diffadd> <diffdel> al. </diffdel> <diffadd> al . </diffadd> <diffdel> N </diffdel> <diffadd> N </diffadd> <diffadd> k </diffadd> <diffdel> k </diffdel> <diffadd> 2 </diffadd> <diffdel> 1 </diffdel> <diffadd> 5 </diffadd> <diffdel> 2 </diffdel> <diffdel> 2 </diffdel> <diffadd> 6 </diffadd> <diffdel> aquatic animals \ food container ( C1 ) flora \ electrical items ( C2 ) fruits \ furniture ( C3 ) insects \ manmade things ( C4 ) animals \ people ( C5 ) outdoor places \ vehicles ( C6 ) </diffdel> <diffadd> IL \ outlier superclasses </diffadd> <diffadd> aquatic animals \ food container ( C1 ) flora \ electrical items ( C2 ) fruits \ furniture ( C3 ) insects \ manmade things ( C4 ) animals \ people ( C5 ) outdoor places \ vehicles ( C6 ) </diffadd> <diffdel> IL \ outlier super - classes animals \ garments ( T1 ) reptiles \ edible items ( T2 ) aquatic animals \ birds ( T 3 ) household items \ nature ( T4 ) vehicles \ miscellaneous ( T5 ) </diffdel> <diffadd> animals \ garments ( T1 ) reptiles \ edible items ( T2 ) aquatic animals \ birds ( T 3 ) household items \ nature ( T4 ) vehicles \ miscellaneous ( T5 ) </diffadd> <diffdel> 2 </diffdel> <diffadd> 6 </diffadd> <diffdel> Gradually changing the deployed scenario : When the prior probabilities of the classes change in the deployed scenario , the BSN quickly re-trains to learn the new IL classes and regains efficiency on the new image stream . Figure 3 illustrates the learning behavior of the BSN when the IL and ùê∂ ! ùê∂ " </diffdel> <diffadd> ùê∂ ! ùê∂ " </diffadd> <diffdel> ) </diffdel> <diffadd> Gradually changing the deployed scenario : When the prior probabilities of the classes change in the deployed scenario , the BSN quickly re-trains to learn the new IL classes and regains efficiency on the new image stream . Figure 3 illustrates the learning behavior of the BSN when the IL and </diffadd> <diffdel> 0 . </diffdel> <diffadd> 0. </diffadd> <diffadd> ( a ) ( b ) Figure 5 : ( a ) Comparison of ENVISE with SOTA OL detection methods , and ( b ) Comparison of different BSN architectures in terms of GiE on different superclasses of the CIFAR - 100 dataset . </diffadd> <diffdel> ( a ) ( b ) Figure 5 : ( a ) Comparison of ENVISE with SOTA OL detection methods , and ( b ) Comparison of different BSN architectures in terms of GiE on different superclasses of the CIFAR - 100 dataset . </diffdel> <diffdel> Nin </diffdel> <diffadd> Nin </diffadd> <diffdel> + ( 3 ) </diffdel> <diffadd> + </diffadd> <diffadd> ( 7 ) </diffadd> <diffdel> Y + q√óX </diffdel> <diffadd> GiE = </diffadd> <diffdel> i ( 4 ) </diffdel> <diffdel> GiE = </diffdel> <diffdel> Nin </diffdel> <diffadd> NT </diffadd> <diffdel> FL </diffdel> <diffadd> F Ltr </diffadd> <diffadd> Y + q√óX Nin NT i </diffadd> <diffdel> L </diffdel> <diffadd> Lin </diffadd> <diffdel> L </diffdel> <diffadd> Lo </diffadd> <diffdel> NT tr NT in o </diffdel> <diffadd> ( 8 ) </diffadd> To numerically compute GiE , we use X = 9048 ( TN ) , Y = 6 ( BSN ) , Ntr = 10 and Nin = 30 , while p and q are calculated from Table 1 . However , as discussed previously , Nin can be a very large value since the inference phase can occur for a very long duration . Figure 5 illustrates that ENVISE outperforms SOTA OL detection methods in terms of GiE for different scenarios on CIFAR100 dataset . Thus , the Lat ensures that the BSN can accurately distinguish between representations of IL and OL classes which enables ENVISE to operate in an efficient manner . 5 C ONCLUSION We propose ENVISE , an online distillation framework which uses a BSN to adaptively learn relevant knowledge from a TN to quickly classify the frequent classes in the deployed scenario . In doing so , it automatically allocates processing resources ( i.e. either the BSN or the TN ) to reduce overall computation . To learn proper representations of the IL classes , we propose an attention triplet loss that enables the BSN to learn the semantically relevant regions of the image that are emphasized by the TN . This enables the BSN to accurately distinguish between representations of IL and OL classes which is key for maintaining overall accuracy . Our experiments show that the BSN i) quickly converges to the performance of the TN , ii ) classifies IL classes more accurately than a RvSN and other variants of the BSN , and iii ) distinguishes between IL and OL classes with lower FPR and detection error than other SOTA OL detection methods on CIFAR100 and tiny - imagenet datasets . We introduce a new metric GiE to assess the overall gain in efficiency , and experimentally show that the attention triplet loss enables ENVISE to achieve higher GiE than SOTA OL detection methods . We show that ENVISE is agnostic to the specific TN and BSN and achieves high gains with different BSN and TN architectures .
                  </p>
               </div>

            </div>
            </p>
         </div>
      </section>
   </body>
</HTML>
