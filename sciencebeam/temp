<TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title level="a" type="main" coords="1,108.43,82.34,302.20,14.93">CROSS-SUPERVISED OBJECT DETECTION</title></titleStmt></fileDesc><profileDesc><abstract><p coords="1,143.87,194.01,324.27,151.10">After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this novel learning paradigm cross-supervised object detection. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.</p></abstract></profileDesc><note type="O" coords="1,113.98,117.78,130.22,19.99">Anonymous authors Paper under double-blind review</note></teiHeader><text><body><div><head coords="1,108.30,381.79,97.69,10.37" n="1">INTRODUCTION</head><p coords="1,108.00,426.49,396.00,107.27">Deep architectures have achieved great success in many computer vision tasks including object recognition and the closely related problem of object detection. Modern detectors, such as the Faster RCNN (<ref type="bibr" target="#b8" coords="1,143.71,426.49,65.24,8.64">Ren et al. (2015)</ref>), YOLO (<ref type="bibr" target="#b17" coords="1,256.26,426.49,82.51,8.64">Redmon et al. (2016)</ref>), and RetinaNet (<ref type="bibr" target="#b13" coords="1,415.68,426.49,63.58,8.64">Lin et al. (2017)</ref>), use the same network backbone as popular recognition models. However, even with the same backbone architectures, detection and recognition models require different types of supervision. A good detec- tor relies heavily on precise bounding boxes and labels for each instance (we shall refer to these as instance-level annotations) , whereas a recognition model needs only image-level labels. Needless to say, it is more time consuming and expensive to obtain high quality bounding box annotations than class labels. As a result, current detectors are limited to a small set of categories relative to their object recognition counterparts. To address this limitation, it is natural to ask, " Is it possible to learn detectors with only class labels?" This problem is commonly referred to as weakly supervised object detection (WSOD).</p><p coords="1,398.01,651.64,8.09,8.64">Early WSOD work (<ref type="bibr" target="#b9" coords="1,192.52,542.06,85.00,8.64">Hoffman et al. (2014)</ref>) showed fair performance by directly applying recogni- tion networks to object detection. More recently, researchers have used multiple instance learning methods (<ref type="bibr" target="#b3" coords="1,148.38,563.97,89.79,8.64">Dietterich et al. (1997)</ref>) to recast WSOD as a multi-label classification problem (<ref type="bibr" target="#b1" coords="1,108.00,563.97,396.00,19.60">Bilen &amp; Vedaldi (2016)</ref>). However, these weakly supervised detector perform poorly at localization. Most WSOD experiments have been conducted on the ILSVR (<ref type="bibr" target="#b20" coords="1,347.39,585.89,103.77,8.64">Russakovsky et al. (2015)</ref>) data set, in which images have only a single object, or on the PASCAL VOC (<ref type="bibr" target="#b4" coords="1,381.38,596.85,98.72,8.64">Everingham et al. (2010)</ref>) data set, which has only 20 categories. The simplicity of these data sets limits the number and types of distractors in an image, making localization substantially easier. Learning from only class labels, it is challenging to detect objects at different scales in an image that contains many distractors. In particular, as shown in our experiments, weakly supervised object detectors do not work well in complex multi-object scenes, such as the COCO dataset (<ref type="bibr" target="#b13" coords="1,336.52,651.64,61.49,8.64">Lin et al. (2014)</ref>).</p><p coords="2,108.00,271.00,396.00,52.47">To address this challenge, we propose a new form of learning in which the localization of classes with only object labels (weakly labeled classes) can benefit from other classes that have ground truth bounding boxes (fully labeled classes). We refer to this new learning paradigm as cross-supervised object detection (CSOD). More formally, we define CSOD as follows. At training time, we are given 1) images contain objects from both base and novel classes, 2) both class labels and ground truth bounding boxes for base objects, and 3) only class labels for novel objects. Our goal is to Weakly supervised object detector Cross-supervised object detector detect novel objects. In CSOD, base classes and novel classes are disjoint. Thus, it can be seen as performing fully-supervised detection on the base classes and weakly supervised detection on the novel classes. It has similarities to both transfer learning and semi-supervised learning, since it transfer knowledge from base class to novel class and have more information about some instances than other instances. However, CSOD represents a distinct and novel paradigm for learning.</p><p coords="2,108.00,353.69,396.00,74.39">The current state of affairs for this problem is directly applying weakly supervised object detection methods. However, these approach ignores knowledge about localization learned from base classes and has several drawbacks. As shown in <ref type="figure" target="#fig_0" coords="2,272.26,353.69,22.14,8.64">Fig. 1</ref>, a weakly supervised object detector tends to detect only the most discriminating part of novel objects instead of the whole object. Notice how only the head of the person, and not the whole body, is detected. Another issue is that the localizer for one object (e.g., the horse) may be confused by the occurrence of another object, such as the person on the horse. This example illustrates the gap between detection and recognition: without ground truth bounding boxes, the detector acts like a standard recognition model - focusing on discriminating rather than detecting.</p><p coords="2,108.00,436.38,396.00,129.19">In this paper, we explore two major mechanisms for improving on this. Our first mechanism is unifying detection and recognition. Using the same network backbone architecture, recognition and detection can be seen as image-level classification and region-level classification respectively, sug- gesting a strong relation between them. In particular, it suggests a shared training framework in which the same backbone is used with different heads for detection and recognition. Thus, we com- bine a detection head learned from ground truth bounding boxes, and a recognition head learned in a weakly supervised fashion from class labels. Unlike a traditional recognition head, our recognition head produces a class score for multiple proposals and is capable of detecting objects. The second mechanism is learning a spatial correlation module to reduce the gap between detection and recog- nition. It takes several high-confidence bounding boxes produced by the recognition head as input, and learns to regress ground truth bounding boxes. By combining these mechanisms together, our model outperforms all previous models when all novel objects are weakly labeled.</p><p coords="2,108.00,628.66,396.00,30.55">In summary, our contributions are three-fold. First, we define a new task-cross-supervised object detection, which enables us to leverage knowledge from fully labeled base categories to help learn a robust detector from novel object class labels only. Second, we propose a unified framework in which two heads are learned from class labels and detection labels respectively, along with a spatial correlation module bridging the gap between recognition and detection. Third, we significantly outperform existing methods (<ref type="bibr" target="#b8" coords="2,229.83,628.66,77.43,8.64">Zhang et al. (2018a)</ref>; <ref type="bibr" target="#b22" coords="2,313.83,628.66,52.27,8.64">Tang et al. (2017</ref>; <ref type="bibr" coords="2,385.79,628.66,4.43,8.64">2018</ref>)) on PASCAL VOC and COCO, suggesting that CSOD could be a promising approach for expanding object detection to a much larger number of categories.</p></div><div><head coords="2,108.30,677.62,102.90,10.37" n="2">RELATED WORK</head><p coords="3,108.00,411.22,396.00,63.43">Weakly supervised object detection. WSOD (<ref type="bibr" target="#b12" coords="2,300.30,701.46,77.07,8.64">Kosugi et al. (2019)</ref>; <ref type="bibr" target="#b32" coords="2,384.18,701.46,70.33,8.64">Zeng et al. (2019)</ref>; <ref type="bibr" target="#b19" coords="2,108.00,701.46,396.00,19.60">Yang et al. (2019)</ref>; <ref type="bibr" target="#b6" coords="2,140.59,712.42,69.82,8.64">Wan et al. (2019)</ref>; <ref type="bibr" target="#b0" coords="2,217.86,712.42,72.28,8.64">Arun et al. (2019)</ref>; <ref type="bibr" target="#b6" coords="2,297.59,712.42,69.81,8.64">Wan et al. (2018)</ref>; <ref type="bibr" target="#b8" coords="2,374.86,712.42,82.14,8.64">Zhang et al. (2018b)</ref>; <ref type="bibr" target="#b8" coords="2,108.00,712.42,396.00,19.60">Ren et al. (2020)</ref>; <ref type="bibr" target="#b8" coords="2,140.17,723.38,80.36,8.64">Zhang et al. (2018c)</ref>; <ref type="bibr" target="#b5" coords="2,227.59,723.38,59.38,8.64">Li et al. (2019)</ref>; <ref type="bibr" target="#b5" coords="2,293.99,723.38,72.01,8.64">Gao et al. (2019b)</ref>; <ref type="bibr" target="#b12" coords="2,373.12,723.38,79.43,8.64">Kosugi et al. (2019)</ref>) attempts to Under review as a conference paper at ICLR 2021 learn a detector with only image category labels. Most of these methods adopt the idea of Multiple Instance Learning (<ref type="bibr" target="#b3" coords="3,184.95,356.43,87.51,8.64">Dietterich et al. (1997)</ref>) to recast WSOD as a multi-label classification task. <ref type="bibr" target="#b1" coords="3,108.00,356.43,396.00,19.60">Bilen &amp; Vedaldi (2016)</ref> propose an end-to-end network by modifying a classifier to operate at the level of image regions, serving as a region selector and a classifier simultaneously. <ref type="bibr" target="#b22" coords="3,414.27,378.35,72.29,8.64">Tang et al. (2017)</ref> and <ref type="bibr" target="#b22" coords="3,108.00,389.31,71.59,8.64">Tang et al. (2018)</ref> find that several iterations of online refinement based on the outputs of previous iterations boosts performance. <ref type="bibr" coords="3,233.65,400.26,68.03,8.64">Wei et al. (2018)</ref> and <ref type="bibr" target="#b2" coords="3,321.93,400.26,71.60,8.64">Diba et al. (2017)</ref> use semantic segmentation based on class activation maps (<ref type="bibr" target="#b36" coords="3,236.12,411.22,68.81,8.64">Zhou et al. (2016)</ref>) to help generate tight bounding boxes. However, WSOD methods tend to focus on the most discriminating part of an object and are prone to distrac- tions from co-occurring objects. Detecting a part of the object or distractors represents convergence to a local optimum. Thus, their performance depends heavily on initialization. In comparison, our proposed cross-supervised object detector alleviates the issue of getting trapped in a local optimum by leveraging knowledge learned from fully labeled base categories.</p><note type="fulltext:other" coords="3,165.37,86.07,307.21,167.12">Novel Base Conv Layers RoI Pooling Fc Layers Softmax over classes Softmax over proposals Proposal scores Sum over proposals Image scores fc fc fc Softmax over classes Supervision Recognition Head P r o p o s a l s Person Person Softmax Classification Regression Supervision Detection Head Top-scoring bbox Person Boat Boat</note></div><div><head coords="3,108.00,482.56,108.83,8.96">Cross-supervised object detection</head><note type="fulltext:other" coords="3,249.81,482.56,4.12,8.96">.</note><p coords="3,108.00,614.46,396.00,41.51">There are several previous works using both image-level and instance-level annotations. <ref type="bibr" target="#b13" coords="3,218.77,493.91,73.71,8.64">Kuen et al. (2019)</ref> learned a parameter transferring function between a classifier and a detector, enabling an image-based classification network to be adapted to a region- based classification network. <ref type="bibr" target="#b9" coords="3,232.23,515.83,90.47,8.64">Hoffman et al. (2014)</ref> and <ref type="bibr" target="#b22" coords="3,344.64,515.83,74.43,8.64">Tang et al. (2016)</ref> propose methods of adaptation for knowledge transfer from classification features to detection features. <ref type="bibr" target="#b26" coords="3,108.00,526.79,396.00,19.60">Uijlings et al. (2018)</ref> use a proposal generator trained on base classes to transfer knowledge by leveraging a MIL framework, organized in a semantic hierarchy. <ref type="bibr" target="#b9" coords="3,294.98,548.71,86.35,8.64">Hoffman et al. (2015)</ref> design a three-step framework to learn a feature representation from weakly supervised classes and strongly supervised classes jointly. However, these methods can only perform object localization in single object scenes such as ILSVRC, whereas our method can perform object detection in complex multi-object scenes as well, e.g. COCO. <ref type="bibr" target="#b5" coords="3,157.87,592.54,71.35,8.64">Gao et al. (2019a)</ref> use a few instance-level labels and a large scale of image-level labels for each category in a training-mining framework, which is referred to as semi-supervised detection. <ref type="bibr" target="#b8" coords="3,108.00,614.46,80.63,8.64">Zhang et al. (2018a)</ref> propose a framework named MSD that learn objectness on base categories and use it to reject distractors when learning novel objects. In comparison, our spatial correlation module not only learns objectness, but also refines coarse bounding boxes. Further, our model learns from both base and novel classes instead of only novel classes.</p></div><div><head coords="3,108.30,676.31,228.39,10.37" n="3">CROSS-SUPERVISED OBJECT DETECTION</head><p coords="3,108.00,701.29,396.00,30.55">CSOD requires us to learn from instance-level annotations (detection labels) and image-level anno- tations (recognition labels). In this section, we explain the unification of detection and recognition and introduce our framework. In the next section, we describe our novel spatial correlation module.</p><note type="fulltext:other" coords="4,108.00,28.88,199.79,8.64">Under review as a conference paper at ICLR 2021</note></div><div><head coords="4,108.25,85.34,205.31,8.64" n="3.1">UNIFYING DETECTION AND RECOGNITION</head><p coords="4,108.00,224.99,396.00,52.47">How to learn a detector from both instance-level and image-level annotations? Since detection and recognition can be seen as region-level and image-level classification respectively, a natural choice is to design a unified framework that combines a detection head and a recognition head that can learn from image-level and instance-level annotations respectively. Here we exploit several baselines to unify the detection and recognition head. (1) Finetune. We first learn through the detection head on base classes with fully labeled samples. Then, we finetune our model using the recognition head on novel classes with only class labels. (2) Two Head. We simultaneously learn the detection and recognition head on base and novel classes, respectively. The weights of the backbones are updated using the loss backpropagated from both heads jointly. (3) Two head <hi rend="superscript">+</hi> . Instead of learning only on novel classes, we learn the recognition head from class labels of both base and novel classes whereas the recognition head remain the same. (4) Two Branch. Instead of having two shared fully-connected layers after RoI pooling layer (see <ref type="figure" target="#fig_1" coords="4,409.29,224.99,22.95,8.64">Fig. 2</ref>), we make these two fully-connected layers seperated, allowing the detection and recognition head to have separate unshared pair of fully-connected layers each. Everything else is the same as the Two Head baseline. Experiments are conducted to compare these baselines in § 5.1 and § 5.2. Our proposed model is based on Two Head. We will discuss the details in § 3.2.</p><p coords="4,108.00,318.64,396.00,74.39">The connection between the recognition and detection head. The baselines mentioned above only use the recognition head to detect novel objects, ignoring the fact that a detection head can play the same role even better. A majority of WSOD methods (<ref type="bibr" target="#b22" coords="4,340.45,307.68,67.52,8.64">Tang et al. (2017)</ref>; <ref type="bibr" target="#b6" coords="4,414.52,307.68,67.12,8.64">Wan et al. (2019)</ref>; <ref type="bibr" coords="4,108.00,307.68,396.00,19.60">Wei et al. (2018)</ref>) find that re-train a new detector taking the top-scoring bounding boxes from a weakly supervised object detector as ground truth marginally improve the performance. Even with coarse and noisy pseudo bounding boxes, a standard object detector produces better detection results than a weakly supervised object detector. Keeping this hypothesis in mind, we introduce a guidance from the recognition head to the detection head. For each of the novel categories existing in a training sample, the recognition head outputs the top-scoring bounding box, which are then used by the detection head as supervision in that sample.</p></div><div><head coords="4,108.25,408.83,186.16,8.64" n="3.2">DETECTION-RECOGNITION NETWORK</head><p coords="4,108.00,460.81,396.00,30.55">The structure of our Detection-Recognition Network (DRN) is shown in <ref type="figure" target="#fig_1" coords="4,396.56,427.93,21.85,8.64">Fig. 2</ref>. Given an image, we first generate 2000 object proposals by Selective Search (<ref type="bibr" target="#b26" coords="4,339.33,438.89,82.42,8.64">Uijlings et al. (2013)</ref>) or RPN (<ref type="bibr" target="#b8" coords="4,108.00,438.89,396.00,19.60">Ren et al. (2015)</ref>) trained on base classes. The image and proposals are fed into several convolutional (conv) layers followed by a region-of-interest (RoI) pooling layer (<ref type="bibr" target="#b7" coords="4,354.93,460.81,62.54,8.64">Girshick (2015)</ref>) to output fixed-size feature maps. Then, these feature maps are fed into two fully connected (fc) layers to produce a collection of proposal features, which are further branched into the recognition and detection head.</p></div><div><head coords="4,108.00,499.28,58.92,8.96">Recognition Head</head><note type="fulltext:other" coords="4,182.04,499.28,5.04,8.96">.</note><p coords="4,108.00,510.62,396.00,30.56">We followed previous WSOD methods to design our recognition head. Since OICR (<ref type="bibr" target="#b22" coords="4,138.72,510.62,67.84,8.64">Tang et al. (2017)</ref>) is simple, neat, and commonly being used, we make our recognition head the same as OICR, but with fewer refinement branches to reduce the computation cost. However, our recognition head can be replaced by any WSOD structure as shown in § 5.3.</p><p coords="4,108.00,651.60,396.00,41.83">Within the recognition head as shown in <ref type="figure" target="#fig_1" coords="4,268.63,549.48,21.69,8.64">Fig. 2</ref>, the proposal features are branched into three streams producing three matrices x <hi rend="superscript">c</hi> , x <hi rend="superscript">d</hi> , x <hi rend="superscript">e</hi> ∈ R <hi rend="superscript">C×|R|</hi> , where C is the number of novel classes and |R| is the number of proposals. Then the two matrices x <hi rend="superscript">c</hi> and x <hi rend="superscript">d</hi> are passed through a softmax function over classes and proposals respectively: σ(x <hi rend="superscript">c</hi> ) and σ(x <hi rend="superscript">d</hi> ). A proposal score x <hi rend="superscript">R</hi> <hi rend="subscript">cr</hi> , indicating the score of c <hi rend="superscript">th</hi> novel class for r <hi rend="superscript">th</hi> proposal, corresponds to the respective element of the matrix x <hi rend="superscript">R</hi> = σ(x <hi rend="superscript">c</hi> ) σ(x <hi rend="superscript">d</hi> ), where refers to an element-wise product. Finally, we obtain the image score of c <hi rend="superscript">th</hi> class φ <hi rend="subscript">c</hi> by summing over all proposals: φ <hi rend="subscript">c</hi> = |R| <hi rend="subscript">r=1</hi> x <hi rend="superscript">R</hi> <hi rend="subscript">cr</hi> . Then we calculate a standard multi-class cross-entropy loss as shown in the first term of Eq.1. Another matrix x <hi rend="superscript">e</hi> is passed through a softmax function over classes, the result of which is expresses as a weighted multi-class cross entropy loss as shown in the second term of Eq.1. We set the pseudo label for each proposal r based on its IoU (or overlap) with the top-scoring proposal of c <hi rend="superscript">th</hi> class, y <hi rend="subscript">cr</hi> = 1 if IoU &gt; 0.5 and y <hi rend="subscript">cr</hi> = 0 otherwise. The weight w <hi rend="subscript">r</hi> for each proposal r is its IoU with the top-scoring proposal. The total loss for the recognition head is</p><formula coords="4,147.62,704.14,316.75,31.06">L <hi rend="subscript">rec</hi> = [− C c=1 y <hi rend="subscript">c</hi> logφ <hi rend="subscript">c</hi> + (1 − y <hi rend="subscript">c</hi> )log(1 − φ <hi rend="subscript">c</hi> )] + [− 1 |R| |R| r=1 C+1 c=1 w <hi rend="subscript">r</hi> y <hi rend="subscript">cr</hi> logx <hi rend="superscript">e</hi> <hi rend="subscript">cr</hi> ] <label coords="4,492.38,715.73,11.62,8.64">(1)</label></formula></div><div><head coords="5,108.00,228.50,147.67,8.96">Supervision from our recognition head</head><note type="fulltext:other" coords="5,269.45,228.50,4.59,8.96">.</note><p coords="5,108.00,226.99,396.00,43.41">We use the matrix x <hi rend="superscript">e</hi> to propose pseudos bounding boxes to guide the detection head. Specifically, we select one top-scoring proposal for each object category that appears in the image as a pseudo bounding box, as done in OICR. We introduce the spatial correlation module in § 4, to further refine this pseudo ground truth.</p></div><div><head coords="5,108.00,278.31,49.09,8.96">Detection Head</head><note type="fulltext:other" coords="5,172.20,278.31,5.04,8.96">.</note><p coords="5,108.00,300.62,396.00,30.55">Now that we have pseudo bounding boxes for novel objects and ground truth bounding boxes for base objects, we train our detection head like a standard detector. For simplicity and efficiency, our detection head use the same structure of Faster R-CNN (<ref type="bibr" target="#b8" coords="5,416.76,300.62,64.62,8.64">Ren et al. (2015)</ref>). At inference time, the detection head produces detection results for both base categories and novel categories.</p></div><div><head coords="5,108.30,351.00,255.15,10.37" n="4">LEARNING TO MODEL SPATIAL CORRELATION</head><p coords="5,108.00,375.68,396.00,52.47">Our intuition is that there exists spatial correlation among high-confidence bounding boxes, and such spatial correlation can be captured to predict ground truth bounding boxes. By representing the spatial correlation in a class-agnostic heatmap, we can easily learn a mapping from recognition- based bounding boxes to ground truth bounding boxes for base categories, and then transfer this mapping to novel categories.</p><p coords="5,108.00,480.29,396.00,30.56">Thus, we propose a spatial correlation module (SCM). SCM is used as a guidance refinement technique, taking sets of high-confidence bounding boxes from the recognition head, and corre- spondingly returning pseudo ground truth bounding boxes to the detection head. These pseudo ground truth boxes act as supervision while training on novel categories. The framework of SCM is showed in <ref type="figure" target="#fig_2" coords="5,162.35,480.29,22.76,8.64">Fig. 3</ref>. Within this module, we first generate a class agnostic heatmap based on the high-confidence bounding boxes predicted by our recognition head, and then we perform detection on top of the heatmap.</p></div><div><head coords="5,108.00,518.75,46.13,8.96">Heatmap synthesis</head><note type="fulltext:other" coords="5,186.68,518.75,4.07,8.96">.</note><p coords="5,108.00,519.14,396.00,74.39">We want to capture the information about how the high-confidence bounding boxes interact amongst themselves. Here, we introduce a simple way of achieving this using a class-agnostic heatmap. For each category existing in the image y <hi rend="subscript">c</hi> = 1, c ∈ C, we first threshold and select high-confidence bounding boxes of class c. Then we synthesize a corresponding class- agnostic heatmap, which is essentially a two-channel feature map of the same size as the original image. The value at each pixel is the sum and the maximum of confidence over all selected bounding boxes covering that pixel.</p></div><div><head coords="5,108.00,601.44,45.47,8.96">Heatmap detection</head><note type="fulltext:other" coords="5,186.45,601.44,4.12,8.96">.</note><p coords="5,108.00,601.83,396.00,30.56">We consider each class-agnostic heatmap as a two-channel image, and perform detection on it. Specifically, we learn a class-agnostic detector on base classes, that we further use to produce pseudo ground truth bounding boxes for novel objects.</p><p coords="5,108.00,651.64,396.00,41.51">For this task, we use a lightweight one-stage detector, consisting of only five convolutional lay- ers. We follow the same network architecture and loss as FCOS (<ref type="bibr" target="#b25" coords="5,373.88,651.64,66.87,8.64">Tian et al. (2019)</ref>), replacing the backbone and feature pyramid network with five max pooling layers. In our experiments, we also compare this tiny detector to a baseline: using three fully-connected layers to regress the ground- truth location taking the coordinates of high-confidence bounding boxes as input.</p></div><div><head coords="5,108.00,701.07,38.42,8.96">Loss of DRN</head><note type="fulltext:other" coords="5,108.00,701.07,396.00,31.64;6,108.00,28.88,372.21,224.91">. After introducing our SCM, we can formulate the full loss function for DRN. We use L <hi rend="subscript">rec</hi> , L <hi rend="subscript">det</hi> , and L <hi rend="subscript">scm</hi> to indicate the losses from our recognition head, detection head, and spatial correlation module respectively. λ <hi rend="subscript">rec</hi> , λ <hi rend="subscript">det</hi> , and λ <hi rend="subscript">scm</hi> are the regularization hyperparameters used Under review as a conference paper at ICLR 2021 Base Novel Method mean table dog horse mbike person plant sheep sofa train tv mean OICR 42.1 33.4 29.3 56.3 64.6 8.0 23.5 47.2 47.2 48.3 61.7 42.0 PCL 49.2 51.5 37.3 63.3 63.9 15.8 23.6 48.8 55.3 61.2 62.1 48.3 MSD-VGG16 50.6 14.3 69.3 65.4 69.6 2.4 20.5 54.6 34.3 58.3 54.6 44.3 MSD-Ens 53.4 18.3 70.6 66.7 69.8 3.7 24.7 55.0 37.4 58.3 57.3 46.1 MSD-Ens+FRCN 53.9 15.3 72.0 74.4 65.2 15.4 25.1 53.6 54.4 45.6 61.4 48.2 Weight Transfer 68.4 10.4 61.0 58.0 65.1 19.8 19.5 58.0 50.8 58.6 52.7 45.4 Finetune <hi rend="superscript">*</hi> 71.8 17.8 22.9 15.2 71.2 10.2 15.1 61.7 36.6 21.9 61.3 33.4 Two Head <hi rend="superscript">*</hi> 72.9 60.6 33.2 47.7 70.2 3.9 25.5 52.6 58.4 54.7 64.4 47.1 Two Head <hi rend="superscript">+*</hi> 72.4 44.5 29.5 52.4 68.4 5.1 22.6 53.0 55.5 58.6 64.8 45.4 Two Branch <hi rend="superscript">*</hi> 72.7 57.3 30.2 44.2 68.1 3.0 21.4 52.2 53.5 51.2 59.7 44.1 Ours w/o SCM 71.6 62.3 41.9 38.2 73.0 11.3 26.0 60.6 63.8 70.5 65.3 51.3 Ours 72.9 61.0 57.1 63.5 72.0 19.5 24.2 60.9 58.6 68.5 65.5 55.1 <hi rend="superscript">+3.8</hi> Ours <hi rend="superscript">*</hi> w/o SCM 72.7 66.8 50.4 57.0 71.5 12.1 27.6 57.1 62.7 54.2 64.2 52.4 Ours <hi rend="superscript">*</hi> 72.7 60.9 59.4 70.5 71.0 17.5 24.1 62.0 60.5 62.4 69.1 55.7 <hi rend="superscript">+3.3</hi></note><p coords="6,108.00,323.53,349.58,8.64">to balance the three separate loss functions. We train our DRN using the following loss:</p><formula coords="6,226.83,337.58,157.84,9.65">L = λ <hi rend="subscript">rec</hi> L <hi rend="subscript">rec</hi> + λ <hi rend="subscript">det</hi> L <hi rend="subscript">det</hi> + λ <hi rend="subscript">scm</hi> L <hi rend="subscript">scm</hi> <label coords="6,492.38,337.90,11.62,8.64">(2)</label></formula></div><div><head coords="6,108.30,362.67,91.78,10.37" n="5">EXPERIMENTS</head></div><div><head coords="6,108.25,383.00,88.30,8.64" n="5.1">PASCAL VOC</head><p coords="6,108.00,423.13,396.00,41.83">Setup. PASCAL VOC 2007 and 2012 datasets contain 9, 962 and 22, 531 images respectively for 20 object classes. They are divided into train, val, and test sets. Here we follow previous work (<ref type="bibr" target="#b22" coords="6,112.60,423.44,70.50,8.64">Tang et al. (2017)</ref>) to choose the trainval set (5, 011 images from 2007 and 11, 540 images from 2012). We divide the first 10 classes into base classes and the other 10 classes into novel classes. To evaluate our methods, we calculate mean of Average Precision (mAP) based on the PASCAL criteria, i.e., IOU&gt;0.5 between predicted boxes and ground truths.</p></div><div><head coords="6,108.00,472.87,74.89,8.96">Implementation details</head><note type="fulltext:other" coords="6,205.52,472.87,3.77,8.96">.</note><p coords="6,108.00,484.22,396.00,52.47">All our baselines, competitors and our framework are based on VGG16 (<ref type="bibr" target="#b21" coords="6,112.84,484.22,120.65,8.64">Simonyan &amp; Zisserman (2015)</ref>) followed most of weakly supervised object detection methods. We set λ <hi rend="subscript">rec</hi> = 1, λ <hi rend="subscript">det</hi> = 10, and λ <hi rend="subscript">scm</hi> = 10. We train the whole framework for 20 epochs using SGD with a momentum of 0.9, a weight decay of 0.0005 and a learning rate of 0.001, which is reduced by a factor of 10 at 14 <hi rend="superscript">th</hi> epoch. For a stable learning process, we don't provide supervision from recognition head to detection head in the first 9 epochs.</p></div><div><head coords="6,108.00,544.60,67.72,8.96">Baselines and competitors</head><note type="fulltext:other" coords="6,219.75,544.60,4.40,8.96">.</note><p coords="6,425.17,566.91,8.09,8.64">We compare against several baselines as mentioned in § 3.1, two WSOD methods: OICR (<ref type="bibr" target="#b22" coords="6,210.00,555.95,67.95,8.64">Tang et al. (2017)</ref>) and PCL (<ref type="bibr" target="#b22" coords="6,326.55,555.95,66.62,8.64">Tang et al. (2018)</ref>), and two cross-supervised object detector: MSD (<ref type="bibr" target="#b8" coords="6,201.66,566.91,76.94,8.64">Zhang et al. (2018a)</ref>), weight transfer (<ref type="bibr" target="#b13" coords="6,356.74,566.91,68.42,8.64">Kuen et al. (2019)</ref>).</p></div><div><head coords="6,108.00,583.45,4.19,8.96">Results</head><note type="fulltext:other" coords="6,137.30,583.45,4.19,8.96">.</note><p coords="6,108.00,614.83,396.00,65.33">As shown in <ref type="table" target="#tab_0" coords="6,204.05,583.84,29.35,8.64">Table 1</ref>, our method outperforms all other approaches by a large margin (over 7% relative increase in mAP on novel classes). The results are consistent with our discussion in § 3.1. We note that (1) sharing backbone for the recognition and detection head learns a more discriminative embedding for novel objects. In <ref type="table" target="#tab_0" coords="6,302.77,616.72,28.84,8.64">Table 1</ref>, Two Head <hi rend="superscript">*</hi> boosts the performance by 5 points as compared to only using the recognition head (OICR). (2) A supervision from recognition head to detection head exploits the full potential of a detection model. By adding the supervision (Ours <hi rend="superscript">*</hi> w/o SCM ), the result is improved by 5 points as compared to Two Head. (3) Our spatial correlation module successfully captures the spatial correlation between high-confidence proposals. It further boosts the performance by 3 points.</p></div><div><head coords="6,108.25,693.89,53.08,8.64" n="5.2">COCO</head><p coords="7,108.00,288.34,396.00,41.51">Setup. We train on the COCO train2017 split and test on val2017 split. We simulate the cross- supervised object detection scenario on COCO by splitting the 80 classes into base and novel classes. We use a 20/60 split same as <ref type="bibr" target="#b9" coords="7,225.85,288.34,61.88,8.64">Hu et al. (2018)</ref>, dividing the COCO categories into all the 20 classes contained in PASCAL VOC and the 60 that are not. We refer to these as the 'voc' and 'non-voc' category sets. 'voc→non-voc' indicates that we take 'voc' as our base classes and 'non-voc' as our novel classes. Similarly, we split the first 20 classes into 'twenty' and the last 60 classes into 'sixty'.</p><note type="fulltext:other" coords="7,108.00,28.88,392.98,184.38">Under review as a conference paper at ICLR 2021 non-voc → voc: test on B = {voc} sixty → twenty: test on B = {twenty} method AP AP50 AP75 APS APM APL AP AP50 AP75 APS APM APL Rec. Head 4.0 15.4 0.9 1.2 5.7 5.8 4.7 16.4 1.3 1.7 8.0 6.9 OICR 4.2 15.7 1.0 1.3 5.5 5.9 4.5 16.6 1.4 2.0 8.2 7.1 PCL 9.2 19.6 - - - - 9.2 19.6 - - - - Weight T. 9.3 26.4 5.7 5.8 11.7 12.4 8.7 25.5 5.5 5.4 11.5 11.7 Finetune 2.3 7.4 0.3 0.7 3.1 3.3 2.4 7.7 0.2 0.5 2.8 3.0 Two Head 11.0 30.2 6.1 6.2 15.4 15.4 11.3 29.5 5.8 6.3 14.8 15.0 Two Head <hi rend="superscript">+</hi> 9.1 26.7 5.4 5.5 12.1 12.3 9.0 27.1 5.4 5.7 11.7 11.6 Two Branch 9.4 26.6 5.6 5.7 12.3 12.4 8.5 24.4 4.5 4.3 11.9 11.9 Ours w/o SCM 12.5 33.6 6.6 7.3 19.2 16.4 12.6 32.3 7.8 7.0 19.4 17.4 Ours 13.9 <hi rend="superscript">+1.4</hi> 36.2 <hi rend="superscript">+2.6</hi> 7.7 6.9 18.8 19.9 14.0 <hi rend="superscript">+1.4</hi> 34.5 <hi rend="superscript">+2.2</hi> 8.9 7.1 19.2 20.6</note></div><div><head coords="7,108.00,337.76,74.42,8.96">Implementation details</head><note type="fulltext:other" coords="7,205.05,337.76,3.77,8.96">.</note><p coords="7,108.00,337.83,396.00,30.87">The implementation details are the same as § 5.1 by default. We train the whole framework for 13 epochs. There is no supervision from recognition head to detection head in the first 5 epochs. The learning rate is reduced by a factor of 10 at 8 <hi rend="superscript">th</hi> , and 12 <hi rend="superscript">th</hi> epochs.</p></div><div><head coords="7,108.00,376.62,66.20,8.96">Baselines and competitors</head><note type="fulltext:other" coords="7,218.24,376.62,4.40,8.96">.</note><p coords="7,108.00,376.69,396.00,19.92">Most baselines and competitors are the same as § 5.1. 'Rec. Head' represents only using our recognition head structure as a weakly supervised object detector.</p></div><div><head coords="7,108.00,404.51,4.19,8.96">Results</head><note type="fulltext:other" coords="7,137.30,404.51,4.19,8.96">.</note><p coords="7,108.00,404.58,396.00,19.92">The results on COCO still support our discussion in § 5.1. Even in complex multi objects scenes, our DRN outperforms all baselines and competitors by a large margin.</p><note type="fulltext:other" coords="7,112.03,456.03,389.21,251.86">non-voc→voc sixty→twenty method AP50 on B AP50 on B max 35.5 33.8 sum 36.0 34.0 num 31.5 29.5 max+sum 36.2 34.5 max+num 35.7 34.1 sum+num 35.9 34.2 max+sum+num 36.1 34.2 (a) Ablation on Heatmap synthesis. The re- sult suggests using two-channel heatmap consists of maximum confidence and sum of confidence over proposals covering that position. non-voc→voc sixty→twenty method AP50 on B AP50 on B Fc layer 2 layer 31.0 28.7 3 layer 30.8 28.3 4 layer 30.5 28.5 R-50-FPN 36.4 34.8 4 conv 35.8 33.8 FCOS 5 conv 36.2 34.5 w/o SCM 33.6 32.3 (b) Ablation on the structure of SCM. FCOS with 5 conv layers has nearly the best performance and very few parameters compared to a ResNet-50 back- bone. non-voc→voc sixty→twenty method AP50 on B AP50 on B WSDDN 35.7 33.8 OICR 36.6 34.7 Ours 36.4 34.5 (c) Ablation on the structure of the recognition head. OICR has more refinement branches so it be- haves a little better than our recognition head but takes double the computation time. base→novel dataset method AP50 on A AP50 on B RPN 76.2 46.1 PASCAL VOC SS 72.7 55.7 non-voc→voc RPN 46.3 36.2 SS 42.5 34.5 (d) Ablation on the proposal generator. On PAS- CAL VOC, there are not enough categories to learn a good RPN. So, we use selective search and RPN to generate proposals for PASCAL VOC and COCO respectively.</note><note type="fulltext:other" coords="8,119.94,92.26,18.77,162.34">SCM Recognition Head Detection Head</note></div><div><head coords="8,108.25,331.55,132.67,8.64" n="5.3">ABLATION EXPERIMENTS</head></div><div><head coords="8,108.00,350.68,46.68,8.96">Heatmap synthesis</head><note type="fulltext:other" coords="8,187.23,350.68,4.07,8.96">.</note><p coords="8,108.00,351.06,396.00,41.51">In <ref type="table" coords="8,208.50,351.06,32.50,8.64">Table 3a</ref>, we compare the different methods to synthesize the heatmaps in the spatial correlation module. For each position in the heatmap, we consider three kinds of values: the maximum of confidence, the sum of confidence, and the number of proposals covering the position. This result informs us to use max and sum to create a two-channel heatmap.</p></div><div><head coords="8,108.00,400.49,61.20,8.96">Structure of SCM</head><note type="fulltext:other" coords="8,181.51,400.49,6.16,8.96">.</note><p coords="8,108.00,422.48,396.00,19.92">In <ref type="table" coords="8,201.94,400.88,29.03,8.64">Table 3b</ref>, we compare different implementations of SCM. We compare the FCOS (<ref type="bibr" target="#b25" coords="8,139.50,411.84,66.50,8.64">Tian et al. (2019)</ref>) with 5 convolutional layers and the standard FCOS with a ResNet-50 (<ref type="bibr" target="#b8" coords="8,108.00,411.84,396.00,19.60">He et al. (2016)</ref>) backbone. We also compare to the regression baseline mentioned in § 4. Considering the computation cost, we choose FCOS with 5 convolutional layer as our heatmap detector.</p></div><div><head coords="8,108.00,450.30,126.10,8.96">Structure of the Recognition head</head><note type="fulltext:other" coords="8,247.88,450.30,4.59,8.96">.</note><p coords="8,108.00,461.65,396.00,19.60">In <ref type="table" coords="8,265.78,450.69,27.92,8.64">Table 3c</ref>, we compare different structures for the recognition head. WSDDN (<ref type="bibr" target="#b1" coords="8,179.92,461.65,95.29,8.64">Bilen &amp; Vedaldi (2016)</ref>) and OICR are compared to our structure. The results support that our model can benefit from a stronger recognition head.</p><p coords="8,108.00,489.54,396.00,30.56">Different proposal generation methods. <ref type="table" coords="8,277.52,489.54,34.24,8.64">Table 3d</ref> shows the ablation of different ways to generate proposals. In PASCAL VOC with only 10 base classes, RPN performs worse than selective search. In COCO with 60 base classes, RPN performs better than selective search.</p><p coords="8,108.00,528.40,396.00,107.27">Visualization. <ref type="figure" target="#fig_3" coords="8,170.34,528.40,23.66,8.64">Fig. 4</ref> shows detection results on novel objects. Images in the first row, the second row, and the third row are detected by our model from the recognition head, the SCM, and the detection head respectively. The images in the first row tend to focus on the discriminating parts of the objects, e.g. the first and the second images contain only a part of the person. It also tends to detect co-occurring objects, e.g. the fourth image not only detects horse but also a large part of the person. Our SCM alleviates these problems. It tends to focus on the whole object, e.g. the first and the third samples detect the whole person instead of only the head. Also, it can correct unsatisfactory bounding boxes distracted by co-occurring objects, e.g. SCM correctly localizes the horse instead of localizing both the person and the horse in the fourth example. Obviously, bounding boxes in the third row are the best, indicating the efficacy of our framework.</p></div><div><head coords="8,108.30,655.10,87.08,10.37" n="6">CONCLUSION</head><p coords="8,108.00,679.54,396.00,52.47">We focus on a novel learning paradigm-cross-supervised object detection. We explore two major ways to build a good cross-supervised object detector: sharing network backbone between a recog- nition head and a detection head, and learning a spatial correlation module to bridge the gap between recognition and detection. Significant improvement on PASCAL VOC and COCO suggests a novel and promising approach for expanding object detection to a much larger number of categories.</p></div><figure coords="2,108.00,203.28,396.00,41.90" xml:id="fig_0"><head coords="2,108.00,203.66,36.63,8.64">Figure 1:</head><label coords="2,108.00,203.66,36.63,8.64">Figure 1:</label><figDesc coords="2,108.00,203.28,396.00,41.90">A comparison between weakly supervised object detector and our detector. Weakly supervised object detector only detects the most discriminating part of an object, e.g., focus on head of a person when detecting a person; or being distracted by co-occurring instances, e.g., distracted by the person on the horse when detecting a horse. Our detector can address these issues.</figDesc><graphic coords="2,137.04,99.75,159.46,95.82" type="image"/><graphic coords="2,312.89,99.75,159.46,95.82" type="image"/></figure><figure coords="3,108.00,270.75,396.00,52.86" xml:id="fig_1"><head coords="3,108.00,271.13,36.63,8.64">Figure 2:</head><label coords="3,108.00,271.13,36.63,8.64">Figure 2:</label><figDesc coords="3,108.00,270.75,396.00,52.86">Our Detection-Recognition Network (DRN) without the spatial correlation module. In this illustration, Person belongs to novel classes and Boat belongs to base classes. The recogni- tion head learns from the class label Person and outputs the top-scoring bounding box to help the detection head learn to detect the person. The spatial correlation module, discussed in § 4, can be added to further refine the top-scoring bounding boxes.</figDesc><graphic coords="3,278.28,92.24,65.81,43.99" type="image"/><graphic coords="3,278.28,92.24,65.81,43.99" type="image"/><graphic coords="3,135.92,121.21,86.20,57.23" type="image"/><graphic coords="3,135.92,121.21,86.20,57.23" type="image"/><graphic coords="3,371.99,165.92,55.80,37.20" type="image"/><graphic coords="3,371.99,165.92,55.80,37.20" type="image"/><graphic coords="3,136.28,185.23,86.20,57.59" type="image"/><graphic coords="3,136.28,185.23,86.20,57.59" type="image"/></figure><figure coords="5,108.00,176.53,396.00,30.94" xml:id="fig_2"><head coords="5,108.00,176.92,36.76,8.64">Figure 3:</head><label coords="5,108.00,176.92,36.76,8.64">Figure 3:</label><figDesc coords="5,108.00,176.53,396.00,30.94">Our spatial correlation module (SCM). Our SCM learns to capture spatial correlation among high-confidence bounding boxes, generating a class-agnostic heatmap for the whole image. A heatmap detector is then trained to learn ground truth bounding boxes.</figDesc><graphic coords="5,114.66,81.86,382.67,83.91" type="image"/><graphic coords="5,114.66,81.86,382.67,83.91" type="image"/></figure><figure type="table" coords="6,108.00,264.10,396.00,43.41" xml:id="tab_0"><head coords="6,108.00,266.00,32.61,8.64">Table 1:</head><label coords="6,108.00,266.00,32.61,8.64">Table 1:</label><figDesc coords="6,108.00,264.10,396.00,43.41">Object Detection performance (mAP %) on PASCAL VOC 2007 test set. <hi rend="superscript">*</hi> indicates using the structure of OICR in the recognition head. "MSD-Ens" is the ensemble of AlexNet and VGG16. "MSD-Ens+FRCN" indicates using an ensemble model to predict pseudo ground truths and then learn a Fast-RCNN (Girshick (2015)) using VGG-16.</figDesc></figure><figure type="table" coords="7,108.00,226.07,396.00,30.94" xml:id="tab_1"><head coords="7,108.00,226.46,32.90,8.64">Table 2:</head><label coords="7,108.00,226.46,32.90,8.64">Table 2:</label><figDesc coords="7,108.00,226.07,396.00,30.94">The results on COCO. We compare our method with several strong baselines in § 3.1 and competitors. Our method significantly outperforms these approaches, showing that our cross- supervised object detector is capable of detecting novel objects in complex multi-object scenes.</figDesc></figure><figure type="table" coords="7,224.32,719.70,163.36,9.03" xml:id="tab_2"><head coords="7,224.32,720.09,32.13,8.64">Table 3:</head><label coords="7,224.32,720.09,32.13,8.64">Table 3:</label><figDesc coords="7,259.54,719.70,128.14,8.96">Ablation study of our method.</figDesc></figure><figure coords="8,108.00,279.54,396.00,30.94" xml:id="fig_3"><head coords="8,108.00,279.93,36.94,8.64">Figure 4:</head><label coords="8,108.00,279.93,36.94,8.64">Figure 4:</label><figDesc coords="8,108.00,279.54,396.00,30.94">Detection results on novel objects. The results are from our proposed model but with different heads. The first row shows the results of the recognition head. The second row lists the results from SCM. The third row displays the results from the detection head.</figDesc><graphic coords="8,369.67,81.86,78.81,60.84" type="image"/><graphic coords="8,369.67,81.86,78.81,60.84" type="image"/><graphic coords="8,217.11,82.80,78.89,59.26" type="image"/><graphic coords="8,293.71,82.80,78.63,59.26" type="image"/><graphic coords="8,122.58,83.12,92.65,62.06" type="image"/><graphic coords="8,453.21,83.12,44.13,58.63" type="image"/><graphic coords="8,369.99,145.22,78.81,60.84" type="image"/><graphic coords="8,369.99,145.22,78.81,60.84" type="image"/><graphic coords="8,217.11,145.53,78.89,59.26" type="image"/><graphic coords="8,293.71,145.85,78.63,59.26" type="image"/><graphic coords="8,453.21,145.85,44.13,58.95" type="image"/><graphic coords="8,122.89,146.16,92.65,61.73" type="image"/><graphic coords="8,369.99,208.26,78.81,60.52" type="image"/><graphic coords="8,369.99,208.26,78.81,60.52" type="image"/><graphic coords="8,216.79,208.89,78.89,59.26" type="image"/><graphic coords="8,217.11,208.89,78.14,59.26" type="image"/><graphic coords="8,293.71,208.89,78.63,59.26" type="image"/><graphic coords="8,453.21,208.89,44.13,59.26" type="image"/><graphic coords="8,122.89,209.21,92.65,61.73" type="image"/></figure></body><back><div type="references"><listBibl><note type="O" coords="9,108.30,84.04,66.96,10.37">REFERENCES</note><biblStruct coords="9,108.00,103.19,396.00,30.56" xml:id="b0"><note type="raw_reference" coords="9,108.00,103.19,396.00,30.56">Aditya Arun, C.V. Jawahar, and M. Pawan Kumar. Dissimilarity coefficient based weakly super- vised object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</note><analytic><author><persName coords="9,108.00,103.19,53.36,8.64"><forename type="first" coords="9,108.00,103.19,27.12,8.64">Aditya</forename><surname coords="9,138.40,103.19,4.59,8.64">Arun</surname><note type="O" coords="9,156.77,103.19,4.59,8.64">,</note></persName></author><author><persName coords="9,164.84,103.19,73.16,8.64"><forename type="first" coords="9,164.84,103.19,4.38,8.64">C</forename><note type="O" coords="9,169.22,103.19,4.38,8.64">.</note><forename type="middle" coords="9,173.60,103.19,4.38,8.64">V</forename><note type="O" coords="9,177.99,103.19,4.38,8.64">.</note><surname coords="9,185.65,103.19,4.31,8.64">Jawahar</surname><note type="O" coords="9,215.82,103.19,22.17,8.64">, and</note></persName></author><author><persName coords="9,241.27,103.19,73.52,8.64"><forename type="first" coords="9,241.27,103.19,40.79,8.64">M. Pawan</forename><surname coords="9,285.33,103.19,4.91,8.64">Kumar</surname><note type="O" coords="9,309.88,103.19,4.91,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,103.19,386.04,19.60">Dissimilarity coefficient based weakly super- vised object detection</title></analytic><note type="O" coords="9,204.07,114.15,16.81,8.64">. In</note><monogr><title level="j" coords="9,117.96,113.97,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2019" coords="9,205.03,125.11,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="9,197.67,124.93,4.86,8.59">,</note><note type="O" coords="9,222.96,125.11,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,144.36,396.00,19.78" xml:id="b1"><note type="raw_reference" coords="9,108.00,144.36,396.00,19.78">Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</note><analytic><author><persName coords="9,108.00,144.54,67.84,8.64"><forename type="first" coords="9,108.00,144.54,26.00,8.64">Hakan</forename><surname coords="9,136.93,144.54,21.59,8.64">Bilen</surname><note type="O" coords="9,161.46,144.54,14.39,8.64">and</note></persName></author><author><persName coords="9,178.77,144.54,65.18,8.64"><forename type="first" coords="9,178.77,144.54,29.32,8.64">Andrea</forename><surname coords="9,211.02,144.54,4.12,8.64">Vedaldi</surname><note type="O" coords="9,239.83,144.54,4.12,8.64">.</note></persName></author><title level="a" type="main" coords="9,248.95,144.54,144.19,8.64">Weakly supervised deep detection networks</title></analytic><note type="O" coords="9,423.41,144.54,17.62,8.64">. In</note><monogr><title level="j" coords="9,117.96,144.36,386.03,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2016" coords="9,422.84,155.50,4.48,8.64">2016</date></imprint></monogr><note type="O" coords="9,415.48,155.32,4.86,8.59">,</note><note type="O" coords="9,440.77,155.50,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,174.92,396.00,30.56" xml:id="b2"><note type="raw_reference" coords="9,108.00,174.92,396.00,30.56">Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, and Luc Van Gool. Weakly supervised cascaded convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</note><analytic><author><persName coords="9,108.00,174.92,37.45,8.64"><forename type="first" coords="9,108.00,174.92,12.73,8.64">Ali</forename><surname coords="9,123.60,174.92,4.37,8.64">Diba</surname><note type="O" coords="9,141.08,174.92,4.37,8.64">,</note></persName></author><author><persName coords="9,148.41,174.92,59.15,8.64"><forename type="first" coords="9,148.41,174.92,23.35,8.64">Vivek</forename><surname coords="9,174.63,174.92,4.70,8.64">Sharma</surname><note type="O" coords="9,202.85,174.92,4.70,8.64">,</note></persName></author><author><persName coords="9,210.51,174.92,56.11,8.64"><forename type="first" coords="9,210.51,174.92,12.73,8.64">Ali</forename><surname coords="9,226.11,174.92,4.50,8.64">Pazandeh</surname><note type="O" coords="9,262.12,174.92,4.50,8.64">,</note></persName></author><author><persName coords="9,269.58,174.92,91.99,8.64"><forename type="first" coords="9,269.58,174.92,28.77,8.64">Hamed</forename><surname coords="9,301.22,174.92,3.91,8.64">Pirsiavash</surname><note type="O" coords="9,340.31,174.92,21.25,8.64">, and</note></persName></author><author><persName coords="9,364.43,174.92,59.13,8.64"><forename type="first" coords="9,364.43,174.92,15.49,8.64">Luc</forename><surname coords="9,382.79,174.92,22.83,8.64">Van Gool</surname><note type="O" coords="9,419.07,174.92,4.48,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,174.92,386.04,19.60">Weakly supervised cascaded convolutional networks</title></analytic><note type="O" coords="9,249.08,185.88,18.45,8.64">. In</note><monogr><title level="j" coords="9,117.96,185.70,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2017" coords="9,254.04,196.84,4.48,8.64">2017</date></imprint></monogr><note type="O" coords="9,246.69,196.66,4.86,8.59">,</note><note type="O" coords="9,271.97,196.84,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,216.27,396.00,19.60" xml:id="b3"><note type="raw_reference" coords="9,108.00,216.27,396.00,19.60">Thomas G Dietterich, Richard H Lathrop, and Tomás Lozano-Pérez. Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence, 1997.</note><analytic><author><persName coords="9,108.00,216.27,41.99,8.64"><surname coords="9,108.00,216.27,32.10,8.64">Thomas</surname><forename type="first" coords="9,142.80,216.27,7.19,8.64">G</forename></persName></author><author><persName coords="9,152.69,216.27,42.33,8.64"><surname coords="9,152.69,216.27,3.85,8.64">Dietterich</surname><note type="O" coords="9,191.17,216.27,3.85,8.64">,</note></persName></author><author><persName coords="9,197.77,216.27,41.43,8.64"><surname coords="9,197.77,216.27,31.54,8.64">Richard</surname><forename type="first" coords="9,232.01,216.27,7.19,8.64">H</forename></persName></author><author><persName coords="9,241.91,216.27,80.19,8.64"><surname coords="9,241.91,216.27,4.25,8.64">Lathrop</surname><note type="O" coords="9,271.69,216.27,21.39,8.64">, and</note><forename type="first" coords="9,295.77,216.27,26.32,8.64">Tomás</forename></persName></author><author><persName coords="9,324.80,216.27,57.81,8.64"><surname coords="9,324.80,216.27,35.58,8.64">Lozano-Pérez</surname><note type="O" coords="9,378.16,216.27,4.45,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,216.27,386.04,19.60">Solving the multiple instance problem with axis-parallel rectangles</title></analytic><note type="O" coords="9,264.58,227.23,3.90,8.64">.</note><monogr><title level="j" coords="9,272.06,227.05,41.68,8.59">Artificial intelligence</title><imprint><date type="published" when="1997" coords="9,361.35,227.23,4.48,8.64">1997</date></imprint></monogr><note type="O" coords="9,355.10,227.05,3.76,8.59">,</note><note type="O" coords="9,379.28,227.23,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,246.66,396.00,30.56" xml:id="b4"><note type="raw_reference" coords="9,108.00,246.66,396.00,30.56">Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision (IJCV), 2010.</note><analytic><author><persName coords="9,108.00,246.66,75.92,8.64"><forename type="first" coords="9,108.00,246.66,21.58,8.64">Mark</forename><surname coords="9,132.89,246.66,4.64,8.64">Everingham</surname><note type="O" coords="9,179.28,246.66,4.64,8.64">,</note></persName></author><author><persName coords="9,187.44,246.66,60.01,8.64"><forename type="first" coords="9,187.44,246.66,15.49,8.64">Luc</forename><surname coords="9,206.24,246.66,23.28,8.64">Van Gool</surname><note type="O" coords="9,242.97,246.66,4.48,8.64">,</note></persName></author><author><persName coords="9,250.97,246.66,60.86,8.64"><surname coords="9,250.97,246.66,47.04,8.64">Christopher</surname><forename type="first" coords="9,301.32,246.66,5.26,8.64">K</forename><forename type="middle" coords="9,306.58,246.66,5.26,8.64">I</forename></persName></author><author><persName coords="9,315.15,246.66,60.95,8.64"><surname coords="9,315.15,246.66,4.29,8.64">Williams</surname><note type="O" coords="9,349.48,246.66,4.29,8.64">,</note><forename type="first" coords="9,357.28,246.66,18.82,8.64">John</forename></persName></author><author><persName coords="9,379.41,246.66,77.28,8.64"><surname coords="9,379.41,246.66,4.85,8.64">Winn</surname><note type="O" coords="9,398.79,246.66,22.75,8.64">, and</note><forename type="first" coords="9,424.85,246.66,31.84,8.64">Andrew</forename></persName></author><author><persName coords="9,460.01,246.66,43.99,8.64"><surname coords="9,460.01,246.66,4.40,8.64">Zisserman</surname><note type="O" coords="9,499.60,246.66,4.40,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,257.61,154.51,8.64">The pascal visual object classes (voc) challenge</title></analytic><note type="O" coords="9,305.01,257.61,4.07,8.64">.</note><monogr><title level="j" coords="9,312.11,257.44,187.66,8.59">International journal of computer vision (IJCV)</title><imprint><date type="published" when="2010" coords="9,117.96,268.57,4.48,8.64">2010</date></imprint></monogr><note type="O" coords="9,499.77,257.44,4.23,8.59">,</note><note type="O" coords="9,135.90,268.57,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,288.00,396.00,30.56" xml:id="b5"><note type="raw_reference" coords="9,108.00,288.00,396.00,30.56">Jiyang Gao, Jiang Wang, Shengyang Dai, Li-Jia Li, and Ram Nevatia. Note-rcnn: Noise tolerant ensemble rcnn for semi-supervised object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019a.</note><analytic><author><persName coords="9,108.00,288.00,48.26,8.64"><forename type="first" coords="9,108.00,288.00,26.01,8.64">Jiyang</forename><surname coords="9,137.17,288.00,4.77,8.64">Gao</surname><note type="O" coords="9,151.49,288.00,4.77,8.64">,</note></persName></author><author><persName coords="9,159.60,288.00,49.67,8.64"><forename type="first" coords="9,159.60,288.00,21.03,8.64">Jiang</forename><surname coords="9,183.78,288.00,5.10,8.64">Wang</surname><note type="O" coords="9,204.17,288.00,5.10,8.64">,</note></persName></author><author><persName coords="9,212.60,288.00,64.32,8.64"><forename type="first" coords="9,212.60,288.00,44.27,8.64">Shengyang</forename><surname coords="9,260.04,288.00,4.22,8.64">Dai</surname><note type="O" coords="9,272.70,288.00,4.22,8.64">,</note></persName></author><author><persName coords="9,280.24,288.00,55.47,8.64"><forename type="first" coords="9,280.24,288.00,23.24,8.64">Li-Jia</forename><surname coords="9,306.65,288.00,3.78,8.64">Li</surname><note type="O" coords="9,314.22,288.00,21.50,8.64">, and</note></persName></author><author><persName coords="9,338.87,288.00,54.96,8.64"><forename type="first" coords="9,338.87,288.00,18.82,8.64">Ram</forename><surname coords="9,360.86,288.00,4.12,8.64">Nevatia</surname><note type="O" coords="9,389.71,288.00,4.12,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,288.00,386.04,19.60">Note-rcnn: Noise tolerant ensemble rcnn for semi-supervised object detection</title></analytic><note type="O" coords="9,325.65,298.96,18.39,8.64">. In</note><monogr><title level="j" coords="9,117.96,298.78,386.04,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="9,282.33,309.92,4.47,8.64">2019a</date></imprint></monogr><note type="O" coords="9,275.30,309.74,4.55,8.59">,</note><note type="O" coords="9,304.70,309.92,4.47,8.64">.</note></biblStruct><biblStruct coords="9,108.00,329.35,396.00,41.51" xml:id="b6"><note type="raw_reference" coords="9,108.00,329.35,396.00,41.51">Yan Gao, Boxiao Liu, Nan Guo, Xiaochun Ye, Fang Wan, Haihang You, and Dongrui Fan. C- midn: Coupled multiple instance detection network with segmentation guidance for weakly su- pervised object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019b.</note><analytic><author><persName coords="9,108.00,329.35,38.37,8.64"><forename type="first" coords="9,108.00,329.35,15.60,8.64">Yan</forename><surname coords="9,127.28,329.35,4.77,8.64">Gao</surname><note type="O" coords="9,141.59,329.35,4.77,8.64">,</note></persName></author><author><persName coords="9,150.35,329.35,48.79,8.64"><forename type="first" coords="9,150.35,329.35,28.78,8.64">Boxiao</forename><surname coords="9,182.81,329.35,4.08,8.64">Liu</surname><note type="O" coords="9,195.06,329.35,4.08,8.64">,</note></persName></author><author><persName coords="9,203.11,329.35,39.92,8.64"><forename type="first" coords="9,203.11,329.35,16.60,8.64">Nan</forename><surname coords="9,223.39,329.35,4.91,8.64">Guo</surname><note type="O" coords="9,238.12,329.35,4.91,8.64">,</note></persName></author><author><persName coords="9,247.02,329.35,55.52,8.64"><forename type="first" coords="9,247.02,329.35,38.73,8.64">Xiaochun</forename><surname coords="9,289.43,329.35,4.37,8.64">Ye</surname><note type="O" coords="9,298.17,329.35,4.37,8.64">,</note></persName></author><author><persName coords="9,306.51,329.35,43.95,8.64"><forename type="first" coords="9,306.51,329.35,19.78,8.64">Fang</forename><surname coords="9,329.97,329.35,5.13,8.64">Wan</surname><note type="O" coords="9,345.34,329.35,5.13,8.64">,</note></persName></author><author><persName coords="9,354.45,329.35,74.34,8.64"><forename type="first" coords="9,354.45,329.35,33.75,8.64">Haihang</forename><surname coords="9,391.88,329.35,4.64,8.64">You</surname><note type="O" coords="9,405.80,329.35,23.00,8.64">, and</note></persName></author><author><persName coords="9,432.47,329.35,54.18,8.64"><forename type="first" coords="9,432.47,329.35,33.21,8.64">Dongrui</forename><surname coords="9,469.36,329.35,4.32,8.64">Fan</surname><note type="O" coords="9,482.33,329.35,4.32,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,329.35,386.04,30.56">C- midn: Coupled multiple instance detection network with segmentation guidance for weakly su- pervised object detection</title></analytic><note type="O" coords="9,218.17,351.26,19.02,8.64">. In</note><monogr><title level="j" coords="9,117.96,351.09,386.04,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="9,179.49,362.22,4.57,8.64">2019b</date></imprint></monogr><note type="O" coords="9,172.45,362.04,4.55,8.59">,</note><note type="O" coords="9,202.32,362.22,4.57,8.64">.</note></biblStruct><biblStruct coords="9,108.00,381.47,396.00,19.78" xml:id="b7"><note type="raw_reference" coords="9,108.00,381.47,396.00,19.78">Ross Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015.</note><analytic><author><persName coords="9,108.00,381.65,58.25,8.64"><forename type="first" coords="9,108.00,381.65,19.38,8.64">Ross</forename><surname coords="9,129.45,381.65,4.09,8.64">Girshick</surname><note type="O" coords="9,162.16,381.65,4.09,8.64">.</note></persName></author><title level="a" type="main" coords="9,168.94,381.65,30.20,8.64">Fast r-cnn</title></analytic><note type="O" coords="9,206.91,381.65,14.86,8.64">. In</note><monogr><title level="j" coords="9,117.96,381.47,386.04,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2015" coords="9,152.27,392.61,4.48,8.64">2015</date></imprint></monogr><note type="O" coords="9,145.24,392.43,4.55,8.59">,</note><note type="O" coords="9,170.21,392.61,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,412.04,396.00,30.56" xml:id="b8"><note type="raw_reference" coords="9,108.00,412.04,396.00,30.56">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</note><analytic><author><persName coords="9,108.00,412.04,51.30,8.64"><forename type="first" coords="9,108.00,412.04,34.87,8.64">Kaiming</forename><surname coords="9,145.19,412.04,4.70,8.64">He</surname><note type="O" coords="9,154.59,412.04,4.70,8.64">,</note></persName></author><author><persName coords="9,161.66,412.04,64.58,8.64"><forename type="first" coords="9,161.66,412.04,34.31,8.64">Xiangyu</forename><surname coords="9,198.29,412.04,4.66,8.64">Zhang</surname><note type="O" coords="9,221.58,412.04,4.66,8.64">,</note></persName></author><author><persName coords="9,228.60,412.04,75.25,8.64"><forename type="first" coords="9,228.60,412.04,37.64,8.64">Shaoqing</forename><surname coords="9,268.56,412.04,4.64,8.64">Ren</surname><note type="O" coords="9,282.46,412.04,21.38,8.64">, and</note></persName></author><author><persName coords="9,306.17,412.04,36.36,8.64"><forename type="first" coords="9,306.17,412.04,16.05,8.64">Jian</forename><surname coords="9,324.54,412.04,4.50,8.64">Sun</surname><note type="O" coords="9,338.03,412.04,4.50,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,412.04,386.04,19.60">Deep residual learning for image recog- nition</title></analytic><note type="O" coords="9,140.03,423.00,19.61,8.64">. In</note><monogr><title level="j" coords="9,117.96,422.82,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2016" coords="9,154.49,433.96,4.48,8.64">2016</date></imprint></monogr><note type="O" coords="9,147.13,433.78,4.86,8.59">,</note><note type="O" coords="9,172.42,433.96,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,453.38,396.00,30.56" xml:id="b9"><note type="raw_reference" coords="9,108.00,453.38,396.00,30.56">Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, and Kate Saenko. Lsda: Large scale detection through adaptation. In Advances in Neural Information Processing Systems (NeurIPS), 2014.</note><analytic><author><persName coords="9,108.00,453.38,60.80,8.64"><forename type="first" coords="9,108.00,453.38,18.82,8.64">Judy</forename><surname coords="9,130.59,453.38,4.78,8.64">Hoffman</surname><note type="O" coords="9,164.03,453.38,4.78,8.64">,</note></persName></author><author><persName coords="9,172.91,453.38,81.33,8.64"><forename type="first" coords="9,172.91,453.38,25.83,8.64">Sergio</forename><surname coords="9,202.51,453.38,4.70,8.64">Guadarrama</surname><note type="O" coords="9,249.54,453.38,4.70,8.64">,</note></persName></author><author><persName coords="9,258.35,453.38,57.08,8.64"><forename type="first" coords="9,258.35,453.38,16.60,8.64">Eric</forename><forename type="middle" coords="9,278.72,453.38,5.54,8.64">S</forename><surname coords="9,288.03,453.38,4.56,8.64">Tzeng</surname><note type="O" coords="9,310.86,453.38,4.56,8.64">,</note></persName></author><author><persName coords="9,319.52,453.38,59.40,8.64"><forename type="first" coords="9,319.52,453.38,40.96,8.64">Ronghang</forename><surname coords="9,364.26,453.38,4.89,8.64">Hu</surname><note type="O" coords="9,374.03,453.38,4.89,8.64">,</note></persName></author><author><persName coords="9,383.03,453.38,56.92,8.64"><forename type="first" coords="9,383.03,453.38,14.68,8.64">Jeff</forename><surname coords="9,401.49,453.38,4.81,8.64">Donahue</surname><note type="O" coords="9,435.14,453.38,4.81,8.64">,</note></persName></author><author><persName coords="9,444.04,453.38,59.96,8.64"><forename type="first" coords="9,444.04,453.38,19.38,8.64">Ross</forename><surname coords="9,467.20,453.38,4.09,8.64">Girshick</surname><note type="O" coords="9,499.91,453.38,4.09,8.64">,</note></persName></author><author><persName coords="9,117.96,464.34,76.14,8.64"><forename type="first" coords="9,117.96,464.34,26.31,8.64">Trevor</forename><surname coords="9,146.63,464.34,3.84,8.64">Darrell</surname><note type="O" coords="9,173.50,464.34,20.61,8.64">, and</note></persName></author><author><persName coords="9,196.47,464.34,52.89,8.64"><forename type="first" coords="9,196.47,464.34,18.81,8.64">Kate</forename><surname coords="9,217.64,464.34,4.53,8.64">Saenko</surname><note type="O" coords="9,244.83,464.34,4.53,8.64">.</note></persName></author><title level="a" type="main" coords="9,252.67,464.34,149.08,8.64">Lsda: Large scale detection through adaptation</title></analytic><note type="O" coords="9,437.75,464.34,15.61,8.64">. In</note><monogr><title level="j" coords="9,117.96,464.16,386.04,19.55">Advances in Neural Information Processing Systems (NeurIPS)</title><imprint><date type="published" when="2014" coords="9,324.35,475.30,4.48,8.64">2014</date></imprint></monogr><note type="O" coords="9,317.51,475.12,4.34,8.59">,</note><note type="O" coords="9,342.28,475.30,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,494.73,396.00,30.56" xml:id="b10"><note type="raw_reference" coords="9,108.00,494.73,396.00,30.56">Judy Hoffman, Deepak Pathak, Trevor Darrell, and Kate Saenko. Detector discovery in the wild: Joint multiple instance and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</note><analytic><author><persName coords="9,108.00,494.73,60.17,8.64"><forename type="first" coords="9,108.00,494.73,18.82,8.64">Judy</forename><surname coords="9,129.97,494.73,4.78,8.64">Hoffman</surname><note type="O" coords="9,163.40,494.73,4.78,8.64">,</note></persName></author><author><persName coords="9,171.49,494.73,63.04,8.64"><forename type="first" coords="9,171.49,494.73,30.43,8.64">Deepak</forename><surname coords="9,205.07,494.73,4.21,8.64">Pathak</surname><note type="O" coords="9,230.33,494.73,4.21,8.64">,</note></persName></author><author><persName coords="9,237.84,494.73,77.88,8.64"><forename type="first" coords="9,237.84,494.73,26.31,8.64">Trevor</forename><surname coords="9,267.31,494.73,3.84,8.64">Darrell</surname><note type="O" coords="9,294.18,494.73,21.54,8.64">, and</note></persName></author><author><persName coords="9,318.87,494.73,53.68,8.64"><forename type="first" coords="9,318.87,494.73,18.81,8.64">Kate</forename><surname coords="9,340.83,494.73,4.53,8.64">Saenko</surname><note type="O" coords="9,368.02,494.73,4.53,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,494.73,386.04,19.60">Detector discovery in the wild: Joint multiple instance and representation learning</title></analytic><note type="O" coords="9,320.94,505.69,17.86,8.64">. In</note><monogr><title level="j" coords="9,117.96,505.51,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2015" coords="9,323.60,516.65,4.48,8.64">2015</date></imprint></monogr><note type="O" coords="9,316.25,516.47,4.86,8.59">,</note><note type="O" coords="9,341.53,516.65,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,536.08,396.00,30.56" xml:id="b11"><note type="raw_reference" coords="9,108.00,536.08,396.00,30.56">Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, and Ross Girshick. Learning to segment every thing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</note><analytic><author><persName coords="9,108.00,536.08,58.52,8.64"><forename type="first" coords="9,108.00,536.08,40.96,8.64">Ronghang</forename><surname coords="9,151.85,536.08,4.89,8.64">Hu</surname><note type="O" coords="9,161.63,536.08,4.89,8.64">,</note></persName></author><author><persName coords="9,169.53,536.08,49.82,8.64"><forename type="first" coords="9,169.53,536.08,19.38,8.64">Piotr</forename><surname coords="9,191.81,536.08,3.94,8.64">Dollár</surname><note type="O" coords="9,215.42,536.08,3.94,8.64">,</note></persName></author><author><persName coords="9,222.36,536.08,51.88,8.64"><forename type="first" coords="9,222.36,536.08,34.87,8.64">Kaiming</forename><surname coords="9,260.13,536.08,4.70,8.64">He</surname><note type="O" coords="9,269.53,536.08,4.70,8.64">,</note></persName></author><author><persName coords="9,277.23,536.08,77.31,8.64"><forename type="first" coords="9,277.23,536.08,26.31,8.64">Trevor</forename><surname coords="9,306.45,536.08,3.84,8.64">Darrell</surname><note type="O" coords="9,333.31,536.08,21.23,8.64">, and</note></persName></author><author><persName coords="9,357.44,536.08,59.08,8.64"><forename type="first" coords="9,357.44,536.08,19.38,8.64">Ross</forename><surname coords="9,379.72,536.08,4.09,8.64">Girshick</surname><note type="O" coords="9,412.43,536.08,4.09,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,536.08,386.04,19.60">Learning to segment every thing</title></analytic><note type="O" coords="9,161.08,547.03,15.19,8.64">. In</note><monogr><title level="j" coords="9,117.96,546.86,386.03,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2018" coords="9,154.49,557.99,4.48,8.64">2018</date></imprint></monogr><note type="O" coords="9,147.13,557.81,4.86,8.59">,</note><note type="O" coords="9,172.42,557.99,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,577.42,396.00,30.56" xml:id="b12"><note type="raw_reference" coords="9,108.00,577.42,396.00,30.56">Satoshi Kosugi, Toshihiko Yamasaki, and Kiyoharu Aizawa. Object-aware instance labeling for weakly supervised object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.</note><analytic><author><persName coords="9,108.00,577.42,63.89,8.64"><forename type="first" coords="9,108.00,577.42,29.34,8.64">Satoshi</forename><surname coords="9,140.97,577.42,4.42,8.64">Kosugi</surname><note type="O" coords="9,167.47,577.42,4.42,8.64">,</note></persName></author><author><persName coords="9,175.81,577.42,102.76,8.64"><forename type="first" coords="9,175.81,577.42,39.51,8.64">Toshihiko</forename><surname coords="9,218.94,577.42,4.59,8.64">Yamasaki</surname><note type="O" coords="9,255.69,577.42,22.88,8.64">, and</note></persName></author><author><persName coords="9,282.20,577.42,73.92,8.64"><forename type="first" coords="9,282.20,577.42,37.63,8.64">Kiyoharu</forename><surname coords="9,323.45,577.42,4.67,8.64">Aizawa</surname><note type="O" coords="9,351.45,577.42,4.67,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,577.42,386.04,19.60">Object-aware instance labeling for weakly supervised object detection</title></analytic><note type="O" coords="9,259.80,588.38,19.56,8.64">. In</note><monogr><title level="j" coords="9,117.96,588.20,386.04,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="9,221.83,599.34,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="9,214.80,599.16,4.55,8.59">,</note><note type="O" coords="9,239.76,599.34,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,618.77,396.00,30.56" xml:id="b13"><note type="raw_reference" coords="9,108.00,618.77,396.00,30.56">Jason Kuen, Federico Perazzi, Zhe Lin, Jianming Zhang, and Yap-Peng Tan. Scaling object detection by transferring classification weights. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.</note><analytic><author><persName coords="9,108.00,618.77,47.97,8.64"><forename type="first" coords="9,108.00,618.77,22.14,8.64">Jason</forename><surname coords="9,132.05,618.77,4.78,8.64">Kuen</surname><note type="O" coords="9,151.19,618.77,4.78,8.64">,</note></persName></author><author><persName coords="9,158.00,618.77,68.59,8.64"><forename type="first" coords="9,158.00,618.77,34.86,8.64">Federico</forename><surname coords="9,194.78,618.77,3.98,8.64">Perazzi</surname><note type="O" coords="9,222.62,618.77,3.98,8.64">,</note></persName></author><author><persName coords="9,228.63,618.77,33.73,8.64"><forename type="first" coords="9,228.63,618.77,15.49,8.64">Zhe</forename><surname coords="9,246.03,618.77,4.08,8.64">Lin</surname><note type="O" coords="9,258.28,618.77,4.08,8.64">,</note></persName></author><author><persName coords="9,264.39,618.77,82.81,8.64"><forename type="first" coords="9,264.39,618.77,36.53,8.64">Jianming</forename><surname coords="9,302.84,618.77,4.66,8.64">Zhang</surname><note type="O" coords="9,326.13,618.77,21.08,8.64">, and</note></persName></author><author><persName coords="9,349.11,618.77,57.95,8.64"><forename type="first" coords="9,349.11,618.77,38.84,8.64">Yap-Peng</forename><surname coords="9,389.88,618.77,4.30,8.64">Tan</surname><note type="O" coords="9,402.77,618.77,4.30,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,618.77,386.04,19.60">Scaling object detection by transferring classification weights</title></analytic><note type="O" coords="9,265.09,629.73,18.09,8.64">. In</note><monogr><title level="j" coords="9,117.96,629.55,386.03,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="9,221.83,640.68,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="9,214.80,640.50,4.55,8.59">,</note><note type="O" coords="9,239.76,640.68,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,660.11,396.00,30.56" xml:id="b14"><note type="raw_reference" coords="9,108.00,660.11,396.00,30.56">Xiaoyan Li, Meina Kan, Shiguang Shan, and Xilin Chen. Weakly supervised object detection with segmentation collaboration. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.</note><analytic><author><persName coords="9,108.00,660.11,47.69,8.64"><forename type="first" coords="9,108.00,660.11,33.65,8.64">Xiaoyan</forename><surname coords="9,144.34,660.11,3.78,8.64">Li</surname><note type="O" coords="9,151.91,660.11,3.78,8.64">,</note></persName></author><author><persName coords="9,158.43,660.11,47.23,8.64"><forename type="first" coords="9,158.43,660.11,25.45,8.64">Meina</forename><surname coords="9,186.57,660.11,4.77,8.64">Kan</surname><note type="O" coords="9,200.89,660.11,4.77,8.64">,</note></persName></author><author><persName coords="9,208.39,660.11,79.87,8.64"><forename type="first" coords="9,208.39,660.11,37.64,8.64">Shiguang</forename><surname coords="9,248.72,660.11,4.48,8.64">Shan</surname><note type="O" coords="9,266.65,660.11,21.61,8.64">, and</note></persName></author><author><persName coords="9,290.95,660.11,46.69,8.64"><forename type="first" coords="9,290.95,660.11,20.48,8.64">Xilin</forename><surname coords="9,314.13,660.11,4.70,8.64">Chen</surname><note type="O" coords="9,332.94,660.11,4.70,8.64">.</note></persName></author><title level="a" type="main" coords="9,117.96,660.11,386.04,19.60">Weakly supervised object detection with segmentation collaboration</title></analytic><note type="O" coords="9,224.96,671.07,17.10,8.64">. In</note><monogr><title level="j" coords="9,117.96,670.89,386.04,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="9,179.49,682.03,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="9,172.45,681.85,4.55,8.59">,</note><note type="O" coords="9,197.42,682.03,4.48,8.64">.</note></biblStruct><biblStruct coords="9,108.00,701.46,396.00,30.56" xml:id="b15"><note type="raw_reference" coords="9,108.00,701.46,396.00,30.56">Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV), 2014.</note><analytic><author><persName coords="9,108.00,701.46,57.26,8.64"><forename type="first" coords="9,108.00,701.46,37.64,8.64">Tsung-Yi</forename><surname coords="9,148.94,701.46,4.08,8.64">Lin</surname><note type="O" coords="9,161.18,701.46,4.08,8.64">,</note></persName></author><author><persName coords="9,168.77,701.46,62.24,8.64"><forename type="first" coords="9,168.77,701.46,32.65,8.64">Michael</forename><surname coords="9,204.73,701.46,4.38,8.64">Maire</surname><note type="O" coords="9,226.63,701.46,4.38,8.64">,</note></persName></author><author><persName coords="9,234.51,701.46,64.27,8.64"><forename type="first" coords="9,234.51,701.46,22.51,8.64">Serge</forename><surname coords="9,260.32,701.46,4.27,8.64">Belongie</surname><note type="O" coords="9,294.51,701.46,4.27,8.64">,</note></persName></author><author><persName coords="9,302.29,701.46,50.61,8.64"><forename type="first" coords="9,302.29,701.46,24.35,8.64">James</forename><surname coords="9,329.94,701.46,4.59,8.64">Hays</surname><note type="O" coords="9,348.31,701.46,4.59,8.64">,</note></persName></author><author><persName coords="9,356.41,701.46,57.26,8.64"><forename type="first" coords="9,356.41,701.46,23.80,8.64">Pietro</forename><surname coords="9,383.51,701.46,4.31,8.64">Perona</surname><note type="O" coords="9,409.36,701.46,4.31,8.64">,</note></persName></author><author><persName coords="9,417.17,701.46,63.95,8.64"><forename type="first" coords="9,417.17,701.46,20.52,8.64">Deva</forename><surname coords="9,441.00,701.46,5.01,8.64">Ramanan</surname><note type="O" coords="9,476.10,701.46,5.01,8.64">,</note></persName></author><author><persName coords="9,117.96,701.46,386.04,19.60"><forename type="first" coords="9,484.63,701.46,19.38,8.64">Piotr</forename><surname coords="9,117.96,712.42,3.94,8.64">Dollár</surname><note type="O" coords="9,141.57,712.42,20.87,8.64">, and</note></persName></author><author><persName coords="9,164.99,712.42,82.11,8.64"><forename type="first" coords="9,164.99,712.42,6.65,8.64">C</forename><surname coords="9,174.16,712.42,45.57,8.64">Lawrence Zitnick</surname><note type="O" coords="9,243.19,712.42,3.91,8.64">.</note></persName></author><title level="a" type="main" coords="9,250.83,712.42,151.24,8.64">Microsoft coco: Common objects in context</title></analytic><note type="O" coords="9,425.82,712.42,15.99,8.64">. In</note><monogr><title level="j" coords="9,117.96,712.24,386.04,19.55">Proceedings of the European Conference on Computer Vision (ECCV)</title><imprint><date type="published" when="2014" coords="9,341.10,723.38,4.48,8.64">2014</date></imprint></monogr><note type="O" coords="9,333.67,723.20,4.94,8.59">,</note><note type="O" coords="9,359.04,723.38,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,85.34,396.00,30.56" xml:id="b16"><note type="raw_reference" coords="10,108.00,85.34,396.00,30.56">Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.</note><analytic><author><persName coords="10,108.00,85.34,56.03,8.64"><forename type="first" coords="10,108.00,85.34,37.64,8.64">Tsung-Yi</forename><surname coords="10,147.70,85.34,4.08,8.64">Lin</surname><note type="O" coords="10,159.95,85.34,4.08,8.64">,</note></persName></author><author><persName coords="10,166.17,85.34,49.82,8.64"><forename type="first" coords="10,166.17,85.34,21.03,8.64">Priya</forename><surname coords="10,189.25,85.34,4.46,8.64">Goyal</surname><note type="O" coords="10,211.54,85.34,4.46,8.64">,</note></persName></author><author><persName coords="10,218.15,85.34,58.23,8.64"><forename type="first" coords="10,218.15,85.34,19.38,8.64">Ross</forename><surname coords="10,239.58,85.34,4.09,8.64">Girshick</surname><note type="O" coords="10,272.29,85.34,4.09,8.64">,</note></persName></author><author><persName coords="10,278.52,85.34,67.57,8.64"><forename type="first" coords="10,278.52,85.34,34.87,8.64">Kaiming</forename><surname coords="10,315.45,85.34,4.70,8.64">He</surname><note type="O" coords="10,324.86,85.34,21.23,8.64">, and</note></persName></author><author><persName coords="10,348.15,85.34,48.83,8.64"><forename type="first" coords="10,348.15,85.34,19.38,8.64">Piotr</forename><surname coords="10,369.58,85.34,3.91,8.64">Dollár</surname><note type="O" coords="10,393.07,85.34,3.91,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,85.34,386.03,19.60">Focal loss for dense object detection</title></analytic><note type="O" coords="10,153.08,96.30,19.39,8.64">. In</note><monogr><title level="j" coords="10,176.09,96.12,323.37,8.59">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2017" coords="10,117.96,107.26,4.48,8.64">2017</date></imprint></monogr><note type="O" coords="10,499.45,96.12,4.55,8.59">,</note><note type="O" coords="10,135.90,107.26,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,127.78,396.00,30.56" xml:id="b17"><note type="raw_reference" coords="10,108.00,127.78,396.00,30.56">Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), 2016.</note><analytic><author><persName coords="10,108.00,127.78,66.57,8.64"><forename type="first" coords="10,108.00,127.78,27.12,8.64">Joseph</forename><surname coords="10,138.32,127.78,5.18,8.64">Redmon</surname><note type="O" coords="10,169.39,127.78,5.18,8.64">,</note></persName></author><author><persName coords="10,177.94,127.78,68.28,8.64"><forename type="first" coords="10,177.94,127.78,31.55,8.64">Santosh</forename><surname coords="10,212.69,127.78,4.19,8.64">Divvala</surname><note type="O" coords="10,242.03,127.78,4.19,8.64">,</note></persName></author><author><persName coords="10,249.59,127.78,77.14,8.64"><forename type="first" coords="10,249.59,127.78,19.38,8.64">Ross</forename><surname coords="10,272.16,127.78,4.09,8.64">Girshick</surname><note type="O" coords="10,304.88,127.78,21.85,8.64">, and</note></persName></author><author><persName coords="10,329.92,127.78,48.71,8.64"><forename type="first" coords="10,329.92,127.78,12.73,8.64">Ali</forename><surname coords="10,345.85,127.78,4.10,8.64">Farhadi</surname><note type="O" coords="10,374.53,127.78,4.10,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,127.78,386.04,19.60">You only look once: Unified, real-time object detection</title></analytic><note type="O" coords="10,218.72,138.74,16.35,8.64">. In</note><monogr><title level="j" coords="10,117.96,138.56,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR)</title><imprint><date type="published" when="2016" coords="10,223.56,149.70,4.48,8.64">2016</date></imprint></monogr><note type="O" coords="10,216.21,149.52,4.86,8.59">,</note><note type="O" coords="10,241.50,149.70,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,170.22,396.00,30.56" xml:id="b18"><note type="raw_reference" coords="10,108.00,170.22,396.00,30.56">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems (NeurIPS), 2015.</note><analytic><author><persName coords="10,108.00,170.22,97.40,8.64"><surname coords="10,108.00,170.22,45.37,8.64">Shaoqing Ren</surname><note type="O" coords="10,162.64,170.22,4.64,8.64">,</note><forename type="first" coords="10,170.53,170.22,34.87,8.64">Kaiming</forename></persName></author><author><persName coords="10,208.50,170.22,36.74,8.64"><surname coords="10,208.50,170.22,4.70,8.64">He</surname><note type="O" coords="10,217.91,170.22,4.70,8.64">,</note><forename type="first" coords="10,225.87,170.22,19.38,8.64">Ross</forename></persName></author><author><persName coords="10,248.34,170.22,73.59,8.64"><surname coords="10,248.34,170.22,4.09,8.64">Girshick</surname><note type="O" coords="10,281.06,170.22,21.73,8.64">, and</note><forename type="first" coords="10,305.89,170.22,16.05,8.64">Jian</forename></persName></author><author><persName coords="10,325.04,170.22,17.99,8.64"><surname coords="10,325.04,170.22,4.50,8.64">Sun</surname><note type="O" coords="10,338.53,170.22,4.50,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,170.22,386.04,19.60">Faster r-cnn: Towards real-time object detection with region proposal networks</title></analytic><note type="O" coords="10,276.39,181.18,16.22,8.64">. In</note><monogr><title level="j" coords="10,117.96,181.00,386.04,19.55">Advances in Neural Information Processing Systems (NeurIPS)</title><imprint><date type="published" when="2015" coords="10,163.89,192.14,4.48,8.64">2015</date></imprint></monogr><note type="O" coords="10,157.06,191.96,4.34,8.59">,</note><note type="O" coords="10,181.82,192.14,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,212.67,396.00,41.51" xml:id="b19"><note type="raw_reference" coords="10,108.00,212.67,396.00,41.51">Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Yong Jae Lee, Alexander G Schwing, and Jan Kautz. Instance-aware, context-focused, and memory-efficient weakly supervised object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</note><analytic><author><persName coords="10,108.00,212.67,70.70,8.64"><forename type="first" coords="10,108.00,212.67,49.80,8.64">Zhongzheng</forename><surname coords="10,160.16,212.67,4.64,8.64">Ren</surname><note type="O" coords="10,174.07,212.67,4.64,8.64">,</note></persName></author><author><persName coords="10,181.09,212.67,47.47,8.64"><forename type="first" coords="10,181.09,212.67,31.55,8.64">Zhiding</forename><surname coords="10,215.00,212.67,4.52,8.64">Yu</surname><note type="O" coords="10,224.04,212.67,4.52,8.64">,</note></persName></author><author><persName coords="10,230.94,212.67,64.73,8.64"><forename type="first" coords="10,230.94,212.67,39.29,8.64">Xiaodong</forename><surname coords="10,272.59,212.67,4.61,8.64">Yang</surname><note type="O" coords="10,291.05,212.67,4.61,8.64">,</note></persName></author><author><persName coords="10,298.05,212.67,54.66,8.64"><forename type="first" coords="10,298.05,212.67,35.97,8.64">Ming-Yu</forename><surname coords="10,336.38,212.67,4.08,8.64">Liu</surname><note type="O" coords="10,348.63,212.67,4.08,8.64">,</note></persName></author><author><persName coords="10,355.09,212.67,55.91,8.64"><forename type="first" coords="10,355.09,212.67,21.04,8.64">Yong</forename><forename type="middle" coords="10,378.49,212.67,12.72,8.64">Jae</forename><surname coords="10,393.58,212.67,4.36,8.64">Lee</surname><note type="O" coords="10,406.65,212.67,4.36,8.64">,</note></persName></author><author><persName coords="10,117.96,212.67,386.04,19.60"><forename type="first" coords="10,413.38,212.67,41.34,8.64">Alexander</forename><forename type="middle" coords="10,457.09,212.67,7.19,8.64">G</forename><surname coords="10,466.64,212.67,4.67,8.64">Schwing</surname><note type="O" coords="10,117.96,212.67,386.04,19.60">, and</note></persName></author><author><persName coords="10,134.93,223.63,42.14,8.64"><forename type="first" coords="10,134.93,223.63,13.28,8.64">Jan</forename><surname coords="10,150.79,223.63,4.38,8.64">Kautz</surname><note type="O" coords="10,172.69,223.63,4.38,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,223.63,386.04,19.60">Instance-aware, context-focused, and memory-efficient weakly supervised object detection</title></analytic><note type="O" coords="10,153.08,234.59,16.84,8.64">. In</note><monogr><title level="j" coords="10,117.96,234.41,386.03,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><biblScope unit="page">2020</biblScope></imprint></monogr><note type="O" coords="10,147.13,245.37,4.86,8.59">,</note><note type="O" coords="10,172.42,245.54,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,266.07,396.00,41.51" xml:id="b20"><note type="raw_reference" coords="10,108.00,266.07,396.00,41.51">Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.</note><analytic><author><persName coords="10,108.00,266.07,77.64,8.64"><forename type="first" coords="10,108.00,266.07,19.32,8.64">Olga</forename><surname coords="10,131.61,266.07,4.50,8.64">Russakovsky</surname><note type="O" coords="10,181.14,266.07,4.50,8.64">,</note></persName></author><author><persName coords="10,190.38,266.07,39.44,8.64"><forename type="first" coords="10,190.38,266.07,11.07,8.64">Jia</forename><surname coords="10,205.75,266.07,4.81,8.64">Deng</surname><note type="O" coords="10,225.01,266.07,4.81,8.64">,</note></persName></author><author><persName coords="10,234.56,266.07,33.90,8.64"><forename type="first" coords="10,234.56,266.07,16.60,8.64">Hao</forename><surname coords="10,255.46,266.07,4.34,8.64">Su</surname><note type="O" coords="10,264.13,266.07,4.34,8.64">,</note></persName></author><author><persName coords="10,273.21,266.07,70.42,8.64"><forename type="first" coords="10,273.21,266.07,35.42,8.64">Jonathan</forename><surname coords="10,312.92,266.07,4.39,8.64">Krause</surname><note type="O" coords="10,339.24,266.07,4.39,8.64">,</note></persName></author><author><persName coords="10,348.38,266.07,73.49,8.64"><forename type="first" coords="10,348.38,266.07,31.29,8.64">Sanjeev</forename><surname coords="10,383.96,266.07,4.21,8.64">Satheesh</surname><note type="O" coords="10,417.66,266.07,4.21,8.64">,</note></persName></author><author><persName coords="10,426.61,266.07,39.43,8.64"><forename type="first" coords="10,426.61,266.07,19.37,8.64">Sean</forename><surname coords="10,450.27,266.07,5.26,8.64">Ma</surname><note type="O" coords="10,460.79,266.07,5.26,8.64">,</note></persName></author><author><persName coords="10,117.96,266.07,386.04,19.60"><forename type="first" coords="10,470.80,266.07,33.21,8.64">Zhiheng</forename><surname coords="10,117.96,277.03,4.84,8.64">Huang</surname><note type="O" coords="10,142.17,277.03,4.84,8.64">,</note></persName></author><author><persName coords="10,149.76,277.03,69.23,8.64"><forename type="first" coords="10,149.76,277.03,27.67,8.64">Andrej</forename><surname coords="10,180.13,277.03,4.32,8.64">Karpathy</surname><note type="O" coords="10,214.67,277.03,4.32,8.64">,</note></persName></author><author><persName coords="10,221.74,277.03,60.52,8.64"><forename type="first" coords="10,221.74,277.03,27.12,8.64">Aditya</forename><surname coords="10,251.55,277.03,4.39,8.64">Khosla</surname><note type="O" coords="10,277.88,277.03,4.39,8.64">,</note></persName></author><author><persName coords="10,285.01,277.03,76.03,8.64"><forename type="first" coords="10,285.01,277.03,32.65,8.64">Michael</forename><surname coords="10,320.36,277.03,4.07,8.64">Bernstein</surname><note type="O" coords="10,356.97,277.03,4.07,8.64">,</note></persName></author><author><persName coords="10,363.79,277.03,94.68,8.64"><forename type="first" coords="10,363.79,277.03,41.34,8.64">Alexander</forename><forename type="middle" coords="10,407.83,277.03,4.57,8.64">C</forename><note type="O" coords="10,412.40,277.03,4.57,8.64">.</note><surname coords="10,419.66,277.03,4.34,8.64">Berg</surname><note type="O" coords="10,437.00,277.03,21.47,8.64">, and</note></persName></author><author><persName coords="10,461.17,277.03,42.83,8.64"><forename type="first" coords="10,461.17,277.03,8.86,8.64">Li</forename><surname coords="10,472.73,277.03,19.55,8.64">Fei-Fei</surname><note type="O" coords="10,500.09,277.03,3.91,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,287.99,176.19,8.64">ImageNet Large Scale Visual Recognition Challenge</title></analytic><note type="O" coords="10,328.47,287.99,4.29,8.64">.</note><monogr><title level="j" coords="10,117.96,287.81,386.04,19.55">International Journal of Computer Vision (IJCV)</title><imprint><date type="published" when="2015" coords="10,150.05,298.95,4.48,8.64">2015</date></imprint></monogr><note type="O" coords="10,143.33,298.77,4.23,8.59">,</note><note type="O" coords="10,167.98,298.95,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,319.47,396.00,30.56" xml:id="b21"><note type="raw_reference" coords="10,108.00,319.47,396.00,30.56">Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.</note><analytic><author><persName coords="10,108.00,319.47,84.80,8.64"><forename type="first" coords="10,108.00,319.47,24.34,8.64">Karen</forename><surname coords="10,135.25,319.47,40.26,8.64">Simonyan</surname><note type="O" coords="10,178.41,319.47,14.39,8.64">and</note></persName></author><author><persName coords="10,195.71,319.47,78.74,8.64"><forename type="first" coords="10,195.71,319.47,31.84,8.64">Andrew</forename><surname coords="10,230.46,319.47,4.40,8.64">Zisserman</surname><note type="O" coords="10,270.06,319.47,4.40,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,319.47,386.04,19.60">Very deep convolutional networks for large-scale image recognition</title></analytic><note type="O" coords="10,161.84,330.43,14.98,8.64">. In</note><monogr><title level="j" coords="10,178.91,330.25,320.70,8.59">Proceedings of the International Conference on Learning Representations (ICLR)</title><imprint><date type="published" when="2015" coords="10,117.96,341.39,4.48,8.64">2015</date></imprint></monogr><note type="O" coords="10,499.61,330.25,4.39,8.59">,</note><note type="O" coords="10,135.90,341.39,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,361.91,396.00,30.56" xml:id="b22"><note type="raw_reference" coords="10,108.00,361.91,396.00,30.56">Peng Tang, Xinggang Wang, Xiang Bai, and Wenyu Liu. Multiple instance detection network with online instance classifier refinement. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</note><analytic><author><persName coords="10,108.00,361.91,44.75,8.64"><forename type="first" coords="10,108.00,361.91,19.93,8.64">Peng</forename><surname coords="10,130.59,361.91,4.43,8.64">Tang</surname><note type="O" coords="10,148.32,361.91,4.43,8.64">,</note></persName></author><author><persName coords="10,155.46,361.91,67.39,8.64"><forename type="first" coords="10,155.46,361.91,39.24,8.64">Xinggang</forename><surname coords="10,197.36,361.91,5.10,8.64">Wang</surname><note type="O" coords="10,217.75,361.91,5.10,8.64">,</note></persName></author><author><persName coords="10,225.56,361.91,60.42,8.64"><forename type="first" coords="10,225.56,361.91,24.35,8.64">Xiang</forename><surname coords="10,252.57,361.91,4.08,8.64">Bai</surname><note type="O" coords="10,264.81,361.91,21.17,8.64">, and</note></persName></author><author><persName coords="10,288.65,361.91,46.81,8.64"><forename type="first" coords="10,288.65,361.91,27.83,8.64">Wenyu</forename><surname coords="10,319.14,361.91,4.08,8.64">Liu</surname><note type="O" coords="10,331.38,361.91,4.08,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,361.91,386.04,19.60">Multiple instance detection network with online instance classifier refinement</title></analytic><note type="O" coords="10,258.89,372.87,15.61,8.64">. In</note><monogr><title level="j" coords="10,117.96,372.69,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2017" coords="10,254.04,383.83,4.48,8.64">2017</date></imprint></monogr><note type="O" coords="10,246.69,383.65,4.86,8.59">,</note><note type="O" coords="10,271.97,383.83,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,404.36,396.00,30.56" xml:id="b23"><note type="raw_reference" coords="10,108.00,404.36,396.00,30.56">Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, and Alan Yuille. Pcl: Proposal cluster learning for weakly supervised object detection. IEEE transactions on pattern analysis and machine intelligence, 2018.</note><analytic><author><persName coords="10,108.00,404.36,45.40,8.64"><forename type="first" coords="10,108.00,404.36,19.93,8.64">Peng</forename><surname coords="10,131.23,404.36,4.43,8.64">Tang</surname><note type="O" coords="10,148.97,404.36,4.43,8.64">,</note></persName></author><author><persName coords="10,156.92,404.36,68.03,8.64"><forename type="first" coords="10,156.92,404.36,39.24,8.64">Xinggang</forename><surname coords="10,199.47,404.36,5.10,8.64">Wang</surname><note type="O" coords="10,219.85,404.36,5.10,8.64">,</note></persName></author><author><persName coords="10,228.47,404.36,40.12,8.64"><forename type="first" coords="10,228.47,404.36,20.48,8.64">Song</forename><surname coords="10,252.26,404.36,4.08,8.64">Bai</surname><note type="O" coords="10,264.50,404.36,4.08,8.64">,</note></persName></author><author><persName coords="10,272.10,404.36,41.52,8.64"><forename type="first" coords="10,272.10,404.36,15.80,8.64">Wei</forename><surname coords="10,291.21,404.36,4.48,8.64">Shen</surname><note type="O" coords="10,309.14,404.36,4.48,8.64">,</note></persName></author><author><persName coords="10,317.14,404.36,43.99,8.64"><forename type="first" coords="10,317.14,404.36,24.35,8.64">Xiang</forename><surname coords="10,344.80,404.36,4.08,8.64">Bai</surname><note type="O" coords="10,357.05,404.36,4.08,8.64">,</note></persName></author><author><persName coords="10,364.65,404.36,65.37,8.64"><forename type="first" coords="10,364.65,404.36,27.83,8.64">Wenyu</forename><surname coords="10,395.78,404.36,4.08,8.64">Liu</surname><note type="O" coords="10,408.03,404.36,21.99,8.64">, and</note></persName></author><title level="a" type="main" coords="10,117.96,404.36,386.04,19.60">Pcl: Proposal cluster learning for weakly supervised object detection</title></analytic><note type="invalid_author_name" coords="10,433.32,404.36,48.98,8.64">Alan Yuille.</note><note type="O" coords="10,376.00,415.32,3.90,8.64">.</note><monogr><title level="j" coords="10,117.96,415.14,386.04,19.55">IEEE transactions on pattern analysis and machine intelligence</title><imprint><date type="published" when="2018" coords="10,258.01,426.27,4.48,8.64">2018</date></imprint></monogr><note type="O" coords="10,251.76,426.10,3.76,8.59">,</note><note type="O" coords="10,275.94,426.27,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,446.80,396.00,41.51" xml:id="b24"><note type="raw_reference" coords="10,108.00,446.80,396.00,41.51">Yuxing Tang, Josiah Wang, Boyang Gao, Emmanuel Dellandréa, Robert Gaizauskas, and Liming Chen. Large scale semi-supervised object detection using visual and semantic knowledge transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</note><analytic><author><persName coords="10,108.00,446.80,54.20,8.64"><forename type="first" coords="10,108.00,446.80,28.78,8.64">Yuxing</forename><surname coords="10,140.02,446.80,4.44,8.64">Tang</surname><note type="O" coords="10,157.76,446.80,4.44,8.64">,</note></persName></author><author><persName coords="10,165.62,446.80,53.63,8.64"><forename type="first" coords="10,165.62,446.80,24.91,8.64">Josiah</forename><surname coords="10,193.77,446.80,5.10,8.64">Wang</surname><note type="O" coords="10,214.16,446.80,5.10,8.64">,</note></persName></author><author><persName coords="10,222.69,446.80,53.22,8.64"><forename type="first" coords="10,222.69,446.80,30.89,8.64">Boyang</forename><surname coords="10,256.82,446.80,4.77,8.64">Gao</surname><note type="O" coords="10,271.14,446.80,4.77,8.64">,</note></persName></author><author><persName coords="10,279.35,446.80,92.60,8.64"><forename type="first" coords="10,279.35,446.80,43.17,8.64">Emmanuel</forename><surname coords="10,325.75,446.80,4.20,8.64">Dellandréa</surname><note type="O" coords="10,367.75,446.80,4.20,8.64">,</note></persName></author><author><persName coords="10,375.38,446.80,96.04,8.64"><forename type="first" coords="10,375.38,446.80,27.12,8.64">Robert</forename><surname coords="10,405.74,446.80,4.35,8.64">Gaizauskas</surname><note type="O" coords="10,449.25,446.80,22.16,8.64">, and</note></persName></author><author><persName coords="10,117.96,446.80,386.04,19.60"><forename type="first" coords="10,474.66,446.80,29.34,8.64">Liming</forename><surname coords="10,117.96,457.76,4.70,8.64">Chen</surname><note type="O" coords="10,136.78,457.76,4.70,8.64">.</note></persName></author><title level="a" type="main" coords="10,144.07,457.76,331.16,8.64">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title></analytic><note type="O" coords="10,117.96,457.76,386.04,19.60">. In</note><monogr><title level="j" coords="10,129.65,468.54,369.49,8.59">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2016" coords="10,117.96,479.68,4.48,8.64">2016</date></imprint></monogr><note type="O" coords="10,499.14,468.54,4.86,8.59">,</note><note type="O" coords="10,135.90,479.68,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,500.20,396.00,30.56" xml:id="b25"><note type="raw_reference" coords="10,108.00,500.20,396.00,30.56">Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.</note><analytic><author><persName coords="10,108.00,500.20,37.98,8.64"><forename type="first" coords="10,108.00,500.20,13.84,8.64">Zhi</forename><surname coords="10,125.57,500.20,4.08,8.64">Tian</surname><note type="O" coords="10,141.90,500.20,4.08,8.64">,</note></persName></author><author><persName coords="10,150.02,500.20,62.13,8.64"><forename type="first" coords="10,150.02,500.20,35.97,8.64">Chunhua</forename><surname coords="10,189.73,500.20,4.48,8.64">Shen</surname><note type="O" coords="10,207.67,500.20,4.48,8.64">,</note></persName></author><author><persName coords="10,216.19,500.20,62.29,8.64"><forename type="first" coords="10,216.19,500.20,16.60,8.64">Hao</forename><surname coords="10,236.53,500.20,4.70,8.64">Chen</surname><note type="O" coords="10,255.34,500.20,23.14,8.64">, and</note></persName></author><author><persName coords="10,282.22,500.20,38.08,8.64"><forename type="first" coords="10,282.22,500.20,20.23,8.64">Tong</forename><surname coords="10,306.19,500.20,4.70,8.64">He</surname><note type="O" coords="10,315.59,500.20,4.70,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,500.20,386.04,19.60">Fcos: Fully convolutional one-stage object detection</title></analytic><note type="O" coords="10,153.08,511.16,19.39,8.64">. In</note><monogr><title level="j" coords="10,176.09,510.98,323.37,8.59">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="10,117.96,522.12,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="10,499.45,510.98,4.55,8.59">,</note><note type="O" coords="10,135.90,522.12,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,542.64,396.00,30.56" xml:id="b26"><note type="raw_reference" coords="10,108.00,542.64,396.00,30.56">Jasper Uijlings, Stefan Popov, and Vittorio Ferrari. Revisiting knowledge transfer for training ob- ject class detectors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</note><analytic><author><persName coords="10,108.00,542.64,62.51,8.64"><forename type="first" coords="10,108.00,542.64,24.90,8.64">Jasper</forename><surname coords="10,135.91,542.64,3.84,8.64">Uijlings</surname><note type="O" coords="10,166.66,542.64,3.84,8.64">,</note></persName></author><author><persName coords="10,173.63,542.64,73.05,8.64"><forename type="first" coords="10,173.63,542.64,25.35,8.64">Stefan</forename><surname coords="10,202.00,542.64,4.53,8.64">Popov</surname><note type="O" coords="10,224.63,542.64,22.05,8.64">, and</note></persName></author><author><persName coords="10,249.68,542.64,63.56,8.64"><forename type="first" coords="10,249.68,542.64,30.95,8.64">Vittorio</forename><surname coords="10,283.64,542.64,3.70,8.64">Ferrari</surname><note type="O" coords="10,309.54,542.64,3.70,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,542.64,386.04,19.60">Revisiting knowledge transfer for training ob- ject class detectors</title></analytic><note type="O" coords="10,193.55,553.60,19.33,8.64">. In</note><monogr><title level="j" coords="10,117.96,553.42,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2018" coords="10,205.03,564.56,4.48,8.64">2018</date></imprint></monogr><note type="O" coords="10,197.67,564.38,4.86,8.59">,</note><note type="O" coords="10,222.96,564.56,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,585.09,396.00,19.60" xml:id="b27"><note type="raw_reference" coords="10,108.00,585.09,396.00,19.60">Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. International journal of computer vision (IJCV), 2013.</note><analytic><author><persName coords="10,108.00,585.09,78.53,8.64"><surname coords="10,108.00,585.09,47.78,8.64">Jasper Rr Uijlings</surname><note type="O" coords="10,182.68,585.09,3.84,8.64">,</note></persName></author><author><persName coords="10,189.49,585.09,37.39,8.64"><surname coords="10,189.49,585.09,21.23,8.64">Koen</surname><forename type="first" coords="10,213.60,585.09,6.64,8.64">E</forename><forename type="middle" coords="10,220.24,585.09,6.64,8.64">A</forename></persName></author><author><persName coords="10,229.75,585.09,83.13,8.64"><surname coords="10,229.75,585.09,37.32,8.64">Van De Sande</surname><note type="O" coords="10,284.96,585.09,4.47,8.64">,</note><forename type="first" coords="10,292.41,585.09,20.47,8.64">Theo</forename></persName></author><author><persName coords="10,315.76,585.09,99.88,8.64"><surname coords="10,315.76,585.09,4.33,8.64">Gevers</surname><note type="O" coords="10,341.74,585.09,21.67,8.64">, and</note><forename type="first" coords="10,366.29,585.09,28.22,8.64">Arnold</forename><forename type="middle" coords="10,397.38,585.09,18.26,8.64">Wm</forename></persName></author><author><persName coords="10,418.51,585.09,44.55,8.64"><surname coords="10,418.51,585.09,4.46,8.64">Smeulders</surname><note type="O" coords="10,458.61,585.09,4.46,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,585.09,386.04,19.60">Selective search for object recognition</title></analytic><note type="O" coords="10,230.73,596.05,3.99,8.64">.</note><monogr><title level="j" coords="10,238.30,595.87,188.93,8.59">International journal of computer vision (IJCV)</title><imprint><date type="published" when="2013" coords="10,433.94,596.05,4.48,8.64">2013</date></imprint></monogr><note type="O" coords="10,427.23,595.87,4.23,8.59">,</note><note type="O" coords="10,451.88,596.05,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,616.57,396.00,30.56" xml:id="b28"><note type="raw_reference" coords="10,108.00,616.57,396.00,30.56">Fang Wan, Pengxu Wei, Jianbin Jiao, Zhenjun Han, and Qixiang Ye. Min-entropy latent model for weakly supervised object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</note><analytic><author><persName coords="10,108.00,616.57,42.97,8.64"><forename type="first" coords="10,108.00,616.57,19.78,8.64">Fang</forename><surname coords="10,130.47,616.57,5.13,8.64">Wan</surname><note type="O" coords="10,145.84,616.57,5.13,8.64">,</note></persName></author><author><persName coords="10,153.71,616.57,50.87,8.64"><forename type="first" coords="10,153.71,616.57,29.89,8.64">Pengxu</forename><surname coords="10,186.29,616.57,4.57,8.64">Wei</surname><note type="O" coords="10,200.00,616.57,4.57,8.64">,</note></persName></author><author><persName coords="10,207.32,616.57,50.02,8.64"><forename type="first" coords="10,207.32,616.57,28.78,8.64">Jianbin</forename><surname coords="10,238.80,616.57,3.71,8.64">Jiao</surname><note type="O" coords="10,253.63,616.57,3.71,8.64">,</note></persName></author><author><persName coords="10,260.08,616.57,72.11,8.64"><forename type="first" coords="10,260.08,616.57,33.21,8.64">Zhenjun</forename><surname coords="10,295.97,616.57,4.77,8.64">Han</surname><note type="O" coords="10,310.29,616.57,21.90,8.64">, and</note></persName></author><author><persName coords="10,334.88,616.57,47.90,8.64"><forename type="first" coords="10,334.88,616.57,32.10,8.64">Qixiang</forename><surname coords="10,369.67,616.57,4.37,8.64">Ye</surname><note type="O" coords="10,378.41,616.57,4.37,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,616.57,386.04,19.60">Min-entropy latent model for weakly supervised object detection</title></analytic><note type="O" coords="10,256.50,627.53,16.03,8.64">. In</note><monogr><title level="j" coords="10,117.96,627.35,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2018" coords="10,254.04,638.49,4.48,8.64">2018</date></imprint></monogr><note type="O" coords="10,246.69,638.31,4.86,8.59">,</note><note type="O" coords="10,271.97,638.49,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,659.01,396.00,30.56" xml:id="b29"><note type="raw_reference" coords="10,108.00,659.01,396.00,30.56">Fang Wan, Chang Liu, Wei Ke, Xiangyang Ji, Jianbin Jiao, and Qixiang Ye. C-mil: Continuation multiple instance learning for weakly supervised object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</note><analytic><author><persName coords="10,108.00,659.01,43.28,8.64"><forename type="first" coords="10,108.00,659.01,19.78,8.64">Fang</forename><surname coords="10,130.77,659.01,5.13,8.64">Wan</surname><note type="O" coords="10,146.15,659.01,5.13,8.64">,</note></persName></author><author><persName coords="10,154.40,659.01,45.34,8.64"><forename type="first" coords="10,154.40,659.01,26.01,8.64">Chang</forename><surname coords="10,183.41,659.01,4.08,8.64">Liu</surname><note type="O" coords="10,195.65,659.01,4.08,8.64">,</note></persName></author><author><persName coords="10,202.86,659.01,32.65,8.64"><forename type="first" coords="10,202.86,659.01,15.80,8.64">Wei</forename><surname coords="10,221.65,659.01,4.62,8.64">Ke</surname><note type="O" coords="10,230.89,659.01,4.62,8.64">,</note></persName></author><author><persName coords="10,238.64,659.01,55.84,8.64"><forename type="first" coords="10,238.64,659.01,43.72,8.64">Xiangyang</forename><surname coords="10,285.35,659.01,3.04,8.64">Ji</surname><note type="O" coords="10,291.44,659.01,3.04,8.64">,</note></persName></author><author><persName coords="10,297.61,659.01,67.83,8.64"><forename type="first" coords="10,297.61,659.01,28.78,8.64">Jianbin</forename><surname coords="10,329.39,659.01,3.71,8.64">Jiao</surname><note type="O" coords="10,344.22,659.01,21.21,8.64">, and</note></persName></author><author><persName coords="10,368.43,659.01,48.21,8.64"><forename type="first" coords="10,368.43,659.01,32.10,8.64">Qixiang</forename><surname coords="10,403.53,659.01,4.37,8.64">Ye</surname><note type="O" coords="10,412.27,659.01,4.37,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,659.01,386.04,19.60">C-mil: Continuation multiple instance learning for weakly supervised object detection</title></analytic><note type="O" coords="10,381.80,669.97,18.28,8.64">. In</note><monogr><title level="j" coords="10,117.96,669.79,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2019" coords="10,384.10,680.93,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="10,376.75,680.75,4.86,8.59">,</note><note type="O" coords="10,402.03,680.93,4.48,8.64">.</note></biblStruct><biblStruct coords="10,108.00,701.46,396.00,30.56" xml:id="b30"><note type="raw_reference" coords="10,108.00,701.46,396.00,30.56">Yunchao Wei, Zhiqiang Shen, Bowen Cheng, Honghui Shi, Jinjun Xiong, Jiashi Feng, and Thomas Huang. Ts2c: Tight box mining with surrounding segmentation context for weakly supervised object detection. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.</note><analytic><author><persName coords="10,108.00,701.46,55.75,8.64"><forename type="first" coords="10,108.00,701.46,34.86,8.64">Yunchao</forename><surname coords="10,145.46,701.46,4.57,8.64">Wei</surname><note type="O" coords="10,159.18,701.46,4.57,8.64">,</note></persName></author><author><persName coords="10,166.38,701.46,60.99,8.64"><forename type="first" coords="10,166.38,701.46,35.97,8.64">Zhiqiang</forename><surname coords="10,204.96,701.46,4.48,8.64">Shen</surname><note type="O" coords="10,222.89,701.46,4.48,8.64">,</note></persName></author><author><persName coords="10,230.00,701.46,59.08,8.64"><forename type="first" coords="10,230.00,701.46,27.98,8.64">Bowen</forename><surname coords="10,260.58,701.46,4.75,8.64">Cheng</surname><note type="O" coords="10,284.33,701.46,4.75,8.64">,</note></persName></author><author><persName coords="10,291.71,701.46,53.25,8.64"><forename type="first" coords="10,291.71,701.46,34.87,8.64">Honghui</forename><surname coords="10,329.18,701.46,3.95,8.64">Shi</surname><note type="O" coords="10,341.01,701.46,3.95,8.64">,</note></persName></author><author><persName coords="10,347.59,701.46,54.36,8.64"><forename type="first" coords="10,347.59,701.46,24.36,8.64">Jinjun</forename><surname coords="10,374.55,701.46,4.57,8.64">Xiong</surname><note type="O" coords="10,397.38,701.46,4.57,8.64">,</note></persName></author><author><persName coords="10,404.58,701.46,64.73,8.64"><forename type="first" coords="10,404.58,701.46,22.69,8.64">Jiashi</forename><surname coords="10,429.87,701.46,4.48,8.64">Feng</surname><note type="O" coords="10,447.80,701.46,21.50,8.64">, and</note></persName></author><author><persName coords="10,117.96,701.46,386.04,19.60"><forename type="first" coords="10,471.90,701.46,32.10,8.64">Thomas</forename><surname coords="10,117.96,712.42,4.84,8.64">Huang</surname><note type="O" coords="10,142.17,712.42,4.84,8.64">.</note></persName></author><title level="a" type="main" coords="10,117.96,712.42,386.04,19.60">Ts2c: Tight box mining with surrounding segmentation context for weakly supervised object detection</title></analytic><note type="O" coords="10,179.77,723.38,15.49,8.64">. In</note><monogr><title level="j" coords="10,197.61,723.20,276.68,8.59">Proceedings of the European Conference on Computer Vision (ECCV)</title><imprint><date type="published" when="2018" coords="10,481.58,723.38,4.48,8.64">2018</date></imprint></monogr><note type="O" coords="10,474.29,723.20,4.94,8.59">,</note><note type="O" coords="10,499.52,723.38,4.48,8.64">.</note></biblStruct><biblStruct coords="11,108.00,85.34,396.00,30.56" xml:id="b31"><note type="raw_reference" coords="11,108.00,85.34,396.00,30.56">Ke Yang, Dongsheng Li, and Yong Dou. Towards precise end-to-end weakly supervised object detection network. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.</note><analytic><author><persName coords="11,108.00,85.34,38.16,8.64"><forename type="first" coords="11,108.00,85.34,11.37,8.64">Ke</forename><surname coords="11,123.08,85.34,4.61,8.64">Yang</surname><note type="O" coords="11,141.54,85.34,4.61,8.64">,</note></persName></author><author><persName coords="11,150.17,85.34,78.84,8.64"><forename type="first" coords="11,150.17,85.34,45.38,8.64">Dongsheng</forename><surname coords="11,199.27,85.34,3.78,8.64">Li</surname><note type="O" coords="11,206.83,85.34,22.18,8.64">, and</note></persName></author><author><persName coords="11,232.72,85.34,44.40,8.64"><forename type="first" coords="11,232.72,85.34,21.04,8.64">Yong</forename><surname coords="11,257.48,85.34,4.91,8.64">Dou</surname><note type="O" coords="11,272.21,85.34,4.91,8.64">.</note></persName></author><title level="a" type="main" coords="11,117.96,85.34,386.04,19.60">Towards precise end-to-end weakly supervised object detection network</title></analytic><note type="O" coords="11,188.70,96.30,19.70,8.64">. In</note><monogr><title level="j" coords="11,117.96,96.12,386.03,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="11,152.27,107.26,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="11,145.24,107.08,4.55,8.59">,</note><note type="O" coords="11,170.21,107.26,4.48,8.64">.</note></biblStruct><biblStruct coords="11,108.00,126.19,396.00,30.56" xml:id="b32"><note type="raw_reference" coords="11,108.00,126.19,396.00,30.56">Zhaoyang Zeng, Bei Liu, Jianlong Fu, Hongyang Chao, and Lei Zhang. Wsod2: Learning bottom- up and top-down objectness distillation for weakly-supervised object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.</note><analytic><author><persName coords="11,108.00,126.19,65.37,8.64"><forename type="first" coords="11,108.00,126.19,39.74,8.64">Zhaoyang</forename><surname coords="11,150.41,126.19,4.59,8.64">Zeng</surname><note type="O" coords="11,168.78,126.19,4.59,8.64">,</note></persName></author><author><persName coords="11,176.09,126.19,32.84,8.64"><forename type="first" coords="11,176.09,126.19,13.84,8.64">Bei</forename><surname coords="11,192.60,126.19,4.08,8.64">Liu</surname><note type="O" coords="11,204.85,126.19,4.08,8.64">,</note></persName></author><author><persName coords="11,211.65,126.19,49.46,8.64"><forename type="first" coords="11,211.65,126.19,33.76,8.64">Jianlong</forename><surname coords="11,248.09,126.19,4.34,8.64">Fu</surname><note type="O" coords="11,256.77,126.19,4.34,8.64">,</note></persName></author><author><persName coords="11,263.81,126.19,84.81,8.64"><forename type="first" coords="11,263.81,126.19,41.50,8.64">Hongyang</forename><surname coords="11,308.00,126.19,4.70,8.64">Chao</surname><note type="O" coords="11,326.82,126.19,21.81,8.64">, and</note></persName></author><author><persName coords="11,351.30,126.19,43.90,8.64"><forename type="first" coords="11,351.30,126.19,13.28,8.64">Lei</forename><surname coords="11,367.25,126.19,4.66,8.64">Zhang</surname><note type="O" coords="11,390.53,126.19,4.66,8.64">.</note></persName></author><title level="a" type="main" coords="11,117.96,126.19,386.04,19.60">Wsod2: Learning bottom- up and top-down objectness distillation for weakly-supervised object detection</title></analytic><note type="O" coords="11,434.20,137.14,17.44,8.64">. In</note><monogr><title level="j" coords="11,117.96,136.97,386.04,19.55">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title><imprint><date type="published" when="2019" coords="11,386.38,148.10,4.48,8.64">2019</date></imprint></monogr><note type="O" coords="11,379.35,147.92,4.55,8.59">,</note><note type="O" coords="11,404.32,148.10,4.48,8.64">.</note></biblStruct><biblStruct coords="11,108.00,167.03,396.00,19.60" xml:id="b33"><note type="raw_reference" coords="11,108.00,167.03,396.00,19.60">Junge Zhang, Kaiqi Huang, Jianguo Zhang, et al. Mixed supervised object detection with robust objectness transfer. IEEE transactions on pattern analysis and machine intelligence, 2018a.</note><analytic><author><persName coords="11,108.00,167.03,54.47,8.64"><forename type="first" coords="11,108.00,167.03,23.24,8.64">Junge</forename><surname coords="11,134.52,167.03,4.66,8.64">Zhang</surname><note type="O" coords="11,157.81,167.03,4.66,8.64">,</note></persName></author><author><persName coords="11,165.94,167.03,54.47,8.64"><forename type="first" coords="11,165.94,167.03,22.14,8.64">Kaiqi</forename><surname coords="11,191.36,167.03,4.84,8.64">Huang</surname><note type="O" coords="11,215.57,167.03,4.84,8.64">,</note></persName></author><author><persName coords="11,223.88,167.03,85.85,8.64"><forename type="first" coords="11,223.88,167.03,30.99,8.64">Jianguo</forename><surname coords="11,258.16,167.03,4.66,8.64">Zhang</surname><note type="O" coords="11,281.44,167.03,28.29,8.64">, et al.</note></persName></author><title level="a" type="main" coords="11,117.96,167.03,386.04,19.60">Mixed supervised object detection with robust objectness transfer</title></analytic><note type="O" coords="11,190.73,177.99,3.60,8.64">.</note><monogr><title level="j" coords="11,197.91,177.81,211.84,8.59">IEEE transactions on pattern analysis and machine intelligence</title><imprint><date type="published" when="2018" coords="11,457.36,177.99,4.47,8.64">2018a</date></imprint></monogr><note type="O" coords="11,451.11,177.81,3.76,8.59">,</note><note type="O" coords="11,479.72,177.99,4.47,8.64">.</note></biblStruct><biblStruct coords="11,108.00,196.92,396.00,30.56" xml:id="b34"><note type="raw_reference" coords="11,108.00,196.92,396.00,30.56">Xiaopeng Zhang, Jiashi Feng, Hongkai Xiong, and Qi Tian. Zigzag learning for weakly super- vised object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018b.</note><analytic><author><persName coords="11,108.00,196.92,70.36,8.64"><forename type="first" coords="11,108.00,196.92,38.73,8.64">Xiaopeng</forename><surname coords="11,150.41,196.92,4.66,8.64">Zhang</surname><note type="O" coords="11,173.70,196.92,4.66,8.64">,</note></persName></author><author><persName coords="11,182.32,196.92,48.79,8.64"><forename type="first" coords="11,182.32,196.92,22.69,8.64">Jiashi</forename><surname coords="11,208.69,196.92,4.48,8.64">Feng</surname><note type="O" coords="11,226.62,196.92,4.48,8.64">,</note></persName></author><author><persName coords="11,235.08,196.92,83.74,8.64"><forename type="first" coords="11,235.08,196.92,34.31,8.64">Hongkai</forename><surname coords="11,273.06,196.92,4.57,8.64">Xiong</surname><note type="O" coords="11,295.89,196.92,22.93,8.64">, and</note></persName></author><author><persName coords="11,322.49,196.92,34.03,8.64"><forename type="first" coords="11,322.49,196.92,9.96,8.64">Qi</forename><surname coords="11,336.12,196.92,4.08,8.64">Tian</surname><note type="O" coords="11,352.45,196.92,4.08,8.64">.</note></persName></author><title level="a" type="main" coords="11,117.96,196.92,386.04,19.60">Zigzag learning for weakly super- vised object detection</title></analytic><note type="O" coords="11,204.07,207.88,16.81,8.64">. In</note><monogr><title level="j" coords="11,117.96,207.70,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2018" coords="11,205.03,218.84,4.57,8.64">2018b</date></imprint></monogr><note type="O" coords="11,197.67,218.66,4.86,8.59">,</note><note type="O" coords="11,227.86,218.84,4.57,8.64">.</note></biblStruct><biblStruct coords="11,108.00,237.77,396.00,30.56" xml:id="b35"><note type="raw_reference" coords="11,108.00,237.77,396.00,30.56">Yongqiang Zhang, Yancheng Bai, Mingli Ding, Yongqiang Li, and Bernard Ghanem. W2f: A weakly-supervised to fully-supervised framework for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018c.</note><analytic><author><persName coords="11,108.00,237.77,75.04,8.64"><forename type="first" coords="11,108.00,237.77,43.18,8.64">Yongqiang</forename><surname coords="11,155.09,237.77,4.66,8.64">Zhang</surname><note type="O" coords="11,178.38,237.77,4.66,8.64">,</note></persName></author><author><persName coords="11,187.31,237.77,59.65,8.64"><forename type="first" coords="11,187.31,237.77,39.39,8.64">Yancheng</forename><surname coords="11,230.63,237.77,4.08,8.64">Bai</surname><note type="O" coords="11,242.88,237.77,4.08,8.64">,</note></persName></author><author><persName coords="11,251.23,237.77,53.46,8.64"><forename type="first" coords="11,251.23,237.77,27.13,8.64">Mingli</forename><surname coords="11,282.27,237.77,4.48,8.64">Ding</surname><note type="O" coords="11,300.21,237.77,4.48,8.64">,</note></persName></author><author><persName coords="11,308.96,237.77,77.10,8.64"><forename type="first" coords="11,308.96,237.77,43.18,8.64">Yongqiang</forename><surname coords="11,356.06,237.77,3.78,8.64">Li</surname><note type="O" coords="11,363.62,237.77,22.44,8.64">, and</note></persName></author><author><persName coords="11,389.98,237.77,72.26,8.64"><forename type="first" coords="11,389.98,237.77,32.09,8.64">Bernard</forename><surname coords="11,426.00,237.77,5.18,8.64">Ghanem</surname><note type="O" coords="11,457.06,237.77,5.18,8.64">.</note></persName></author><title level="a" type="main" coords="11,117.96,237.77,386.04,19.60">W2f: A weakly-supervised to fully-supervised framework for object detection</title></analytic><note type="O" coords="11,402.86,248.73,20.23,8.64">. In</note><monogr><title level="j" coords="11,117.96,248.55,386.03,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2018" coords="11,408.17,259.69,4.47,8.64">2018c</date></imprint></monogr><note type="O" coords="11,400.82,259.51,4.86,8.59">,</note><note type="O" coords="11,430.54,259.69,4.47,8.64">.</note></biblStruct><biblStruct coords="11,108.00,278.61,396.00,30.55" xml:id="b36"><note type="raw_reference" coords="11,108.00,278.61,396.00,30.55">Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</note><analytic><author><persName coords="11,108.00,278.61,48.42,8.64"><forename type="first" coords="11,108.00,278.61,21.59,8.64">Bolei</forename><surname coords="11,132.90,278.61,4.70,8.64">Zhou</surname><note type="O" coords="11,151.71,278.61,4.70,8.64">,</note></persName></author><author><persName coords="11,159.94,278.61,61.14,8.64"><forename type="first" coords="11,159.94,278.61,27.12,8.64">Aditya</forename><surname coords="11,190.37,278.61,4.39,8.64">Khosla</surname><note type="O" coords="11,216.70,278.61,4.39,8.64">,</note></persName></author><author><persName coords="11,224.60,278.61,69.38,8.64"><forename type="first" coords="11,224.60,278.61,23.74,8.64">Agata</forename><surname coords="11,251.66,278.61,4.23,8.64">Lapedriza</surname><note type="O" coords="11,289.75,278.61,4.23,8.64">,</note></persName></author><author><persName coords="11,297.50,278.61,66.92,8.64"><forename type="first" coords="11,297.50,278.61,21.58,8.64">Aude</forename><surname coords="11,322.38,278.61,4.02,8.64">Oliva</surname><note type="O" coords="11,342.49,278.61,21.92,8.64">, and</note></persName></author><author><persName coords="11,367.73,278.61,71.96,8.64"><forename type="first" coords="11,367.73,278.61,32.66,8.64">Antonio</forename><surname coords="11,403.70,278.61,4.00,8.64">Torralba</surname><note type="O" coords="11,435.69,278.61,4.00,8.64">.</note></persName></author><title level="a" type="main" coords="11,117.96,278.61,386.04,19.60">Learning deep features for discriminative localization</title></analytic><note type="O" coords="11,273.65,289.57,19.11,8.64">. In</note><monogr><title level="j" coords="11,117.96,289.39,386.04,19.55">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title><imprint><date type="published" when="2016" coords="11,281.26,300.53,4.48,8.64">2016</date></imprint></monogr><note type="O" coords="11,273.91,300.35,4.86,8.59">,</note><note type="O" coords="11,299.19,300.53,4.48,8.64">.</note></biblStruct></listBibl></div><div type="annex"><note type="unmatched_graphics"><graphic coords="1,107.00,39.15,397.99,622.13" type="svg"/><graphic coords="2,108.00,39.15,397.00,693.86" type="svg"/><graphic coords="3,107.00,39.15,397.99,584.95" type="svg"/><graphic coords="4,107.00,39.15,397.99,680.57" type="svg"/><graphic coords="5,108.00,39.15,396.00,622.13" type="svg"/><graphic coords="6,108.00,39.15,396.00,587.21" type="svg"/><graphic coords="7,108.00,39.15,397.00,614.00" type="svg"/><graphic coords="8,107.00,39.15,397.99,498.88" type="svg"/></note></div></back></text></TEI>
converted file: /tmp/tmpy22ok6j3-sb-parser/tei.xml
